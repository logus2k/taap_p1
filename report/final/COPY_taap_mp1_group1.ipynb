{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888e9661",
   "metadata": {},
   "source": [
    "# Mini-Project 01 - Implementing a Conditional GAN\n",
    "\n",
    "Advanced Topics in Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "**AntÃ³nio Cruz** (140129), **CÃ¡tia BrÃ¡s** (120093), **Ricardo Kayseller** (95813)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe4d2f",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "---\n",
    "\n",
    "Generative Adversarial Networks (GANs) have become a fundamental class of generative models for learning complex data distributions and synthesizing realistic samples. In the classical (unconditional) GAN framework, the generator receives only random noise as input and learns to produce samples that resemble the training data, while the discriminator learns to distinguish real data from generated samples. Although this setting allows the model to generate realistic images, it provides no control over the semantic content of the generated samples. In the context of datasets with labeled structure, such as MNIST, an unconditional GAN may generate any digit between 0 and 9, but the user cannot specify which digit should be produced.\n",
    "\n",
    "Conditional Generative Adversarial Networks (cGANs) extend the original GAN formulation by incorporating additional side information, such as class labels, into both the generator and the discriminator. By conditioning the generation process on a label ð‘¦, the generator learns a mapping ðº(ð‘§,ð‘¦) that aims to produce samples consistent with the desired class, while the discriminator is trained to assess not only whether an image is real or fake, but also whether it matches the provided condition. This conditioning mechanism enables controlled generation and significantly increases the practical usefulness of GANs in applications where semantic attributes matter, such as digit synthesis, object class control, and, more generally, text-to-image generation.\n",
    "\n",
    "In this project, we investigate the implementation and behavior of conditional GANs on the MNIST dataset of handwritten digits. Starting from an unconditional DCGAN baseline, we progressively introduce conditioning mechanisms that allow explicit control over the generated digit class. Beyond basic conditioning via concatenation of labels, we explore stabilization strategies inspired by the literature, such as Spectral Normalization applied to the discriminator, in order to mitigate training instabilities and mode collapse. The experimental analysis focuses on both qualitative and quantitative aspects, including visual inspection of generated samples, class-conditional control checks, intra-class diversity, and discriminator behavior during training.\n",
    "\n",
    "The main objective of this work is to demonstrate that conditional adversarial training enables controllable image generation, to analyze the limitations of simple conditioning strategies, and to assess how architectural and training refinements improve stability, diversity, and label consistency. This study provides practical insights into the challenges of training cGANs and highlights the importance of appropriate conditioning and regularization techniques for achieving reliable class-conditional generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf50d1",
   "metadata": {},
   "source": [
    "# 2. Environment Setup & Configuration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60594529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required dependencies\n",
    "import os, random, time\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "from threading import Timer\n",
    "\n",
    "import webbrowser\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.kid import KernelInceptionDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7147d6",
   "metadata": {},
   "source": [
    "## 2.1 Global Variables and Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Mode: True for cGAN (conditional), False for DCGAN (unconditional)\n",
    "CONDITIONAL = True\n",
    "\n",
    "LIVE_MONITOR = True\n",
    "LIVE_MONITOR_PORT_NUMBER = 8992\n",
    "EMIT_INTERVAL = 1\n",
    "\n",
    "DATASET_PATH = \"../../dataset/\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 100\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Path to calibrated classifier checkpoint\n",
    "CLASSIFIER_CHECKPOINT = \"classifier/model/mnist_cnn_best.ckpt\"\n",
    "\n",
    "# Example: NUM_STEPS = 15005 corresponds to ~32 epochs (60000 images / 128 batch_size â‰ˆ 469 steps/epoch)\n",
    "NUM_STEPS = 30010\n",
    "SAVE_INTERVAL = 1000\n",
    "\n",
    "# TTUR: Two Time-Scale Update Rule\n",
    "# D learns faster than G, so we use different learning rates\n",
    "LR_D = 4e-4  # Discriminator learning rate\n",
    "LR_G = 1e-4  # Generator learning rate (4x slower)\n",
    "\n",
    "# Adam betas optimized for GAN training\n",
    "# Lower Î²1 (0.5 vs default 0.9) reduces momentum, stabilizes adversarial updates\n",
    "ADAM_BETAS = (0.5, 0.999)\n",
    "\n",
    "# Label smoothing for BCE/LSGAN (use 0.9 instead of 1.0 for real labels)\n",
    "LABEL_SMOOTHING_REAL = 0.9\n",
    "\n",
    "# WGAN-GP: number of critic steps per generator step\n",
    "N_CRITIC = 5\n",
    "\n",
    "# WGAN-GP: gradient penalty coefficient\n",
    "LAMBDA_GP = 10.0\n",
    "\n",
    "# Model output path includes mode subfolder (cgan or dcgan)\n",
    "MODEL_OUTPUT_PATH = f\"model/{'cgan' if CONDITIONAL else 'dcgan'}/\"\n",
    "D_MODEL_NAME = \"D_DRAFT_01\"\n",
    "G_MODEL_NAME = \"G_DRAFT_01\"\n",
    "\n",
    "NUM_EVAL_SAMPLES = 10000\n",
    "\n",
    "# Strategies to benchmark\n",
    "BENCHMARK_STRATEGIES = [\"bce\", \"lsgan\", \"hinge\", \"wgan-gp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e67525",
   "metadata": {},
   "source": [
    "## 2.2 Setup and Training Strategy\n",
    "\n",
    "This project adopts a systematic experimental design to analyze the behavior and stability of conditional Generative Adversarial Networks (cGANs) on the MNIST dataset. Beyond implementing a single adversarial formulation, multiple loss strategies are benchmarked, including the standard binary cross-entropy (BCE) loss, Least Squares GAN (LSGAN), hinge loss, and Wasserstein GAN with Gradient Penalty (WGAN-GP). This design choice follows the theoretical and empirical insights discussed in the lecture materials on GAN losses and training stability, which highlight that different adversarial objectives lead to substantially different optimization dynamics, convergence behavior, and robustness to mode collapse. In particular, WGAN-GP is included due to its smoother loss landscape and explicit enforcement of the Lipschitz constraint, which is known to improve training stability compared to classical GAN formulations.\n",
    "\n",
    "Training is performed using the Two Time-Scale Update Rule (TTUR), where the discriminator is updated with a learning rate four times higher than the generator (LR_D = 4Ã—10â»â´, LR_G = 1Ã—10â»â´). This choice is directly motivated by best practices discussed in the GAN training â€œrecipeâ€ guidelines, which emphasize that an overly weak discriminator fails to provide informative gradients to the generator, while an overly strong discriminator can lead to vanishing gradients. The Adam optimizer is used with Î²â‚ = 0.5 and Î²â‚‚ = 0.999, following the DCGAN and subsequent GAN literature, as these hyperparameters are empirically known to stabilize adversarial training by reducing excessive momentum in the discriminator updates.\n",
    "\n",
    "The batch size is set to 128, representing a compromise between training stability and computational efficiency. Larger batch sizes generally provide more stable gradient estimates for both generator and discriminator, while still fitting comfortably within GPU memory constraints. The dimensionality of the latent space is fixed to 100, following common practice in DCGAN-style architectures, which provides sufficient capacity for modeling the variability of handwritten digits without introducing unnecessary complexity.\n",
    "\n",
    "Training is defined in terms of a fixed number of optimization steps rather than epochs. Specifically, the model is trained for 15,005 steps. This value originates from the reference Keras implementation that was converted to PyTorch and corresponds approximately to 32 epochs over the MNIST training set (MNIST contains 60,000 samples, and with a batch size of 128, one epoch corresponds to roughly 469 iterations). The step-based formulation is consistent with how GAN experiments are typically reported in the literature, as it provides finer control over the balance between generator and discriminator updates, enables predictable checkpointing and sampling intervals, and facilitates fair comparisons across different training configurations. The additional offset of five steps ensures that the final checkpoint aligns exactly with the predefined sampling interval of 1,000 steps, allowing the evolution of generated samples to be consistently monitored at fixed milestones throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe64063",
   "metadata": {},
   "source": [
    "## 2.3 Real-Time Monitoring Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIVE_MONITOR:\n",
    "    from bin.gan_monitor import (\n",
    "        start_server, emit_frames, emit_done,\n",
    "        emit_benchmark_start, emit_strategy_start, emit_strategy_end\n",
    "    )\n",
    "    start_server(port=LIVE_MONITOR_PORT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59932440",
   "metadata": {},
   "source": [
    "A real-time monitoring dashboard was developed and integrated into the training pipeline to support continuous inspection of adversarial dynamics during benchmarking. When enabled, a lightweight local server streams training diagnostics such as generator sample grids, discriminator and generator loss trajectories, and per-strategy progress markers. This design aligns with best-practice recommendations for GAN training, where losses alone are often insufficient to assess convergence or detect failure modes. The live monitor improves experimental transparency, facilitates early detection of mode collapse or imbalance between networks, and ensures consistent reporting across multiple loss strategies within the same benchmarking framework.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility across both numpy and pytorch\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED) # to assure reproducibility on numpy (affects functions like np.random.rand, np.random.shuffle, etc.)\n",
    "torch.manual_seed(SEED)  # to assure reproducibility on Torch (affects weight initialization, dropout, data shuffling, etc.)\n",
    "torch.cuda.manual_seed_all(SEED) # usefull when using more than one GPT, otherwise torch.manual_seed is enough\n",
    "\n",
    "# Ensure deterministic behavior in CuDNN (NVIDIA backend for deep learning ops).\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bacc40",
   "metadata": {},
   "source": [
    "# 3. GAN Loss Strategies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Strategies\n",
    "\n",
    "class GANLossStrategy(ABC):\n",
    "    \"\"\"Base class for GAN loss strategies.\"\"\"\n",
    "    use_sigmoid: bool = True\n",
    "    use_label_smoothing: bool = False  # only BCE/LSGAN use this\n",
    "    n_critic: int = 1  # D steps per G step\n",
    "    smooth_real: float = 1.0\n",
    "    \n",
    "    @abstractmethod\n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Discriminator loss for real images.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Discriminator loss for fake images.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Generator loss.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def gradient_penalty(self, real_imgs: Tensor, fake_imgs: Tensor, labels: Tensor) -> Tensor:\n",
    "        \"\"\"Gradient penalty (only used by WGAN-GP).\"\"\"\n",
    "        return torch.tensor(0.0, device=real_imgs.device)\n",
    "    \n",
    "    def set_d_model(self, d_model) -> None:\n",
    "        \"\"\"Set discriminator reference (used by WGAN-GP).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_d_loss(self, d_loss_real: Tensor, d_loss_fake: Tensor, gp: Tensor) -> Tensor:\n",
    "        \"\"\"Combine D losses. Override for strategy-specific formulas.\"\"\"\n",
    "        return 0.5 * (d_loss_real + d_loss_fake) + gp\n",
    "\n",
    "\n",
    "class BCELossStrategy(GANLossStrategy):\n",
    "    \"\"\"Binary Cross-Entropy loss (original GAN) with label smoothing.\"\"\"\n",
    "    use_sigmoid = True\n",
    "    use_label_smoothing = True\n",
    "    \n",
    "    def __init__(self, device, smooth_real: float = 0.9):\n",
    "        self.device = device\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.smooth_real = smooth_real\n",
    "        self._real_labels = None\n",
    "        self._fake_labels = None\n",
    "    \n",
    "    def _ensure_labels(self, batch_size):\n",
    "        if self._real_labels is None or self._real_labels.size(0) != batch_size:\n",
    "            self._real_labels = torch.full((batch_size, 1), self.smooth_real, device=self.device)\n",
    "            self._fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._real_labels)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._fake_labels)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        # G wants D to output 1.0 (no smoothing for G)\n",
    "        ones = torch.ones(output.size(0), 1, device=self.device)\n",
    "        return self.criterion(output, ones)\n",
    "\n",
    "\n",
    "class LSGANLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Least Squares loss with label smoothing.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    use_label_smoothing = True\n",
    "    \n",
    "    def __init__(self, device, smooth_real: float = 0.9):\n",
    "        self.device = device\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.smooth_real = smooth_real\n",
    "        self._real_labels = None\n",
    "        self._fake_labels = None\n",
    "    \n",
    "    def _ensure_labels(self, batch_size):\n",
    "        if self._real_labels is None or self._real_labels.size(0) != batch_size:\n",
    "            self._real_labels = torch.full((batch_size, 1), self.smooth_real, device=self.device)\n",
    "            self._fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._real_labels)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._fake_labels)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        ones = torch.ones(output.size(0), 1, device=self.device)\n",
    "        return self.criterion(output, ones)\n",
    "\n",
    "\n",
    "class HingeLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Hinge loss â€” used in SAGAN, BigGAN.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(F.relu(1.0 - output))\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(F.relu(1.0 + output))\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "\n",
    "\n",
    "class WGANGPLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Wasserstein loss with gradient penalty.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    n_critic = 5  # train D 5 times per G step\n",
    "    \n",
    "    def __init__(self, lambda_gp: float = 10.0):\n",
    "        self.d_model = None\n",
    "        self.lambda_gp = lambda_gp\n",
    "    \n",
    "    def set_d_model(self, d_model) -> None:\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(output)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "    \n",
    "    def gradient_penalty(self, real_imgs: Tensor, fake_imgs: Tensor, labels: Tensor = None) -> Tensor:\n",
    "        if self.d_model is None:\n",
    "            raise RuntimeError(\"d_model not set. Call set_d_model() first.\")\n",
    "        \n",
    "        batch_size = real_imgs.size(0)\n",
    "        \n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=real_imgs.device)\n",
    "        interpolated = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
    "        \n",
    "        d_out = self.d_model(interpolated, labels)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_out,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_out),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        \n",
    "        return self.lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    def compute_d_loss(self, d_loss_real: Tensor, d_loss_fake: Tensor, gp: Tensor) -> Tensor:\n",
    "        \"\"\"WGAN-GP uses d_fake - d_real + gp (no 0.5 averaging).\"\"\"\n",
    "        return d_loss_fake + d_loss_real + gp  # note: d_loss_real is already negated\n",
    "\n",
    "\n",
    "def get_loss_strategy(name: str, device, lambda_gp: float = 10.0) -> GANLossStrategy:\n",
    "    \"\"\"Factory function to get loss strategy by name.\"\"\"\n",
    "    strategies = {\n",
    "        \"bce\": BCELossStrategy,\n",
    "        \"lsgan\": LSGANLossStrategy,\n",
    "        \"hinge\": HingeLossStrategy,\n",
    "        \"wgan-gp\": WGANGPLossStrategy,\n",
    "    }\n",
    "    if name not in strategies:\n",
    "        raise ValueError(f\"Unknown loss strategy: {name}. Options: {list(strategies.keys())}\")\n",
    "    \n",
    "    if name == \"bce\":\n",
    "        return strategies[name](device, smooth_real=LABEL_SMOOTHING_REAL)\n",
    "    if name == \"lsgan\":\n",
    "        return strategies[name](device, smooth_real=LABEL_SMOOTHING_REAL)\n",
    "    if name == \"wgan-gp\":\n",
    "        return strategies[name](lambda_gp=lambda_gp)\n",
    "    return strategies[name]()\n",
    "\n",
    "\n",
    "def get_discriminator(strategy_name: str, device):\n",
    "    \"\"\"\n",
    "    Factory function to get strategy-appropriate Discriminator.\n",
    "    \n",
    "    WGAN-GP uses a specialized discriminator without spectral normalization\n",
    "    or dropout, since the gradient penalty already enforces the Lipschitz\n",
    "    constraint and stochasticity interferes with GP calculation.\n",
    "    \"\"\"\n",
    "    if strategy_name == \"wgan-gp\":\n",
    "        return DiscriminatorWGAN().to(device)\n",
    "    else:\n",
    "        use_sigmoid = (strategy_name == \"bce\")\n",
    "        return Discriminator(use_sigmoid=use_sigmoid).to(device)\n",
    "\n",
    "\n",
    "def get_adam_betas(strategy_name: str):\n",
    "    \"\"\"\n",
    "    Get optimizer betas appropriate for each strategy.\n",
    "    \n",
    "    WGAN-GP uses beta1=0.0 for fresher gradient estimates without\n",
    "    momentum interference, as recommended in the original paper.\n",
    "    \"\"\"\n",
    "    if strategy_name == \"wgan-gp\":\n",
    "        return (0.0, 0.9)\n",
    "    else:\n",
    "        return ADAM_BETAS  # (0.5, 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3805d",
   "metadata": {},
   "source": [
    "## 3.1 GAN Loss Strategies and Benchmarking Framework\n",
    "\n",
    "In order to systematically study the impact of different adversarial objectives on training stability and sample quality, the training pipeline was designed around a modular loss-strategy framework. Instead of hard-coding a single GAN loss, a common abstract interface was defined to encapsulate the discriminator and generator loss components, enabling multiple loss formulations to be benchmarked under identical architectural and optimization conditions.\n",
    "\n",
    "Four widely adopted loss strategies were implemented and evaluated:\n",
    "\n",
    "1. **Binary Cross-Entropy (BCE) Loss**  \n",
    "   This strategy corresponds to the original GAN formulation, where the discriminator is trained as a probabilistic classifier distinguishing real from generated samples. The generator is optimized to maximize the probability that fake samples are classified as real. To improve training stability, label smoothing is applied to real samples, reducing overconfidence in the discriminator and mitigating sharp gradients that may destabilize the adversarial dynamics. This formulation directly reflects the classical minimax game described in the original GAN literature.\n",
    "\n",
    "2. **Least Squares GAN (LSGAN)**  \n",
    "   The LSGAN objective replaces the binary cross-entropy loss with a least-squares regression loss. Instead of learning to output hard binary decisions, the discriminator is encouraged to regress towards continuous target values for real and fake samples. This formulation has been shown to reduce vanishing gradients and produce smoother optimization landscapes, often resulting in more stable convergence during training. As in the BCE strategy, label smoothing is applied to the real targets to further regularize the discriminator.\n",
    "\n",
    "3. **Hinge Loss**  \n",
    "   The hinge loss formulation adopts a margin-based objective, where the discriminator enforces a separation between real and fake samples using a hinge function. Rather than predicting explicit probabilities, the discriminator outputs real-valued scores, and the generator is trained to maximize these scores for generated samples. This loss is commonly used in modern high-performance GAN architectures and aligns with the design patterns discussed in contemporary GAN literature, particularly in combination with architectural constraints such as spectral normalization.\n",
    "\n",
    "4. **Wasserstein GAN with Gradient Penalty (WGAN-GP)**  \n",
    "   The WGAN-GP strategy reformulates the adversarial game in terms of the Wasserstein distance between the real and generated data distributions. Instead of a classifier, the discriminator acts as a critic that assigns scalar scores to samples. To enforce the required Lipschitz continuity constraint, a gradient penalty term is added, penalizing deviations of the gradient norm from unity along linear interpolations between real and fake samples. Additionally, the critic is updated multiple times per generator update, following the two time-scale update principle, to ensure a sufficiently strong approximation of the Wasserstein distance during training.\n",
    "\n",
    "   Further experiments indicated that the degradation observed in the initial WGAN-GP configuration was strongly linked to the simultaneous application of multiple regularization mechanisms. In particular, the combination of Spectral Normalization and Gradient Penalty imposed redundant Lipschitz constraints on the Discriminator, while Dropout further reduced its effective capacity. In WGAN-GP, the discriminator is designed without spectral normalization or dropout, given that the gradient penalty already ensures the Lipschitz constraint, while stochastic components may interfere with the stability and reliability of the penalty term.\n",
    "\n",
    "By implementing these loss strategies within a unified interface, the project enables a fair and controlled comparison of fundamentally different adversarial objectives. All strategies share the same generator and discriminator architectures, optimization settings, and training schedule, ensuring that observed differences in convergence behavior, stability, and sample quality can be attributed primarily to the choice of loss function rather than to confounding implementation details. This benchmarking setup reflects the theoretical and practical considerations emphasized in the course materials, where the choice of adversarial loss is a central factor in the stability and effectiveness of GAN training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a94775",
   "metadata": {},
   "source": [
    "## 3.2 Software Architecture\n",
    "\n",
    "To enable systematic benchmarking of multiple adversarial objectives and network configurations within a single codebase, the training framework was designed around two core software patterns:\n",
    "\n",
    "- **Strategy Pattern**: All loss functions inherit from an abstract GANLossStrategy base class that defines a common interface (d_loss_real, d_loss_fake, g_loss, gradient_penalty). This allows BCE, LSGAN, Hinge, and WGAN-GP to be used interchangeably without modifying the training loop.\n",
    "\n",
    "- **Factory Pattern**: The functions get_loss_strategy(), get_discriminator(), and get_adam_betas() instantiate the appropriate components based on the selected strategy. Notably, WGAN-GP uses a specialized DiscriminatorWGAN architecture (with LayerNorm, no Spectral Normalization or Dropout) since the gradient penalty already enforces the Lipschitz constraint.\n",
    "\n",
    "- **Conditional Flag**: A global CONDITIONAL parameter switches between cGAN mode (label-conditioned generation) and standard DCGAN mode (unconditional). Both Generator and Discriminator classes accept an optional labels argument, enabling the same architecture to support both configurations.\n",
    "\n",
    "![GAN Training Framework Software Architecture](images/gan_software_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f027eb",
   "metadata": {},
   "source": [
    "**Figure 3.1** - Class architecture of the GAN training framework. The Strategy pattern enables interchangeable loss functions, while Factory functions create strategy-appropriate discriminators. The CONDITIONAL flag toggles between cGAN and DCGAN modes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edd84b",
   "metadata": {},
   "source": [
    "# 4. Data Loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: convert PIL image to tensor (scales [0,255] to [0,1]),\n",
    "# then normalize to [-1, 1] range using mean=0.5, std=0.5\n",
    "# Formula: (x - 0.5) / 0.5 = 2x - 1, which maps [0,1] to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset from a local folder\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=DATASET_PATH,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoader handles batching, shuffling, and parallel loading\n",
    "# drop_last=True discards the final incomplete batch so every batch has exactly BATCH_SIZE samples\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac413f3",
   "metadata": {},
   "source": [
    "## 4.1 Data Loading and Preprocessing\n",
    "\n",
    "MNIST images were converted to tensors and normalized to the range [-1, 1] using a mean of 0.5 and standard deviation of 0.5. This preprocessing step is required when the generator outputs images through a Tanh activation, ensuring that real and generated samples lie on the same numerical scale. The training split contains 60,000 images, and batching was performed with shuffling enabled. The DataLoader used drop_last=True to maintain a constant batch size across iterations, simplifying adversarial training dynamics and logging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716cfa9",
   "metadata": {},
   "source": [
    "# 5. Generator & Discriminator\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590216d7",
   "metadata": {},
   "source": [
    "## 5.1 Conditional Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, num_classes=NUM_CLASSES, conditional=CONDITIONAL):\n",
    "        super().__init__()\n",
    "        self.conditional = conditional\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        if conditional:\n",
    "            self.label_embedding = nn.Embedding(num_classes, latent_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Block 1: 7x7 -> 14x14\n",
    "        self.conv1 = nn.Conv2d(128, 128 * 4, kernel_size=3, padding=1)\n",
    "        self.ps1 = nn.PixelShuffle(2)\n",
    "        self.bn1 = nn.BatchNorm2d(128, momentum=0.8)\n",
    "\n",
    "        # Block 2: 14x14 -> 28x28\n",
    "        self.conv2 = nn.Conv2d(128, 64 * 4, kernel_size=3, padding=1)\n",
    "        self.ps2 = nn.PixelShuffle(2)\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=0.8)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self._icnr_init(self.conv1, upscale_factor=2)\n",
    "        self._icnr_init(self.conv2, upscale_factor=2)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _icnr_init(self, conv_layer, upscale_factor):\n",
    "        \"\"\"Specialized initialization to prevent checkerboard/dots.\"\"\"\n",
    "        new_shape = [\n",
    "            conv_layer.out_channels // (upscale_factor**2),\n",
    "            conv_layer.in_channels,\n",
    "            conv_layer.kernel_size[0],\n",
    "            conv_layer.kernel_size[1]\n",
    "        ]\n",
    "        sub_kernel = torch.randn(new_shape) * 0.02\n",
    "        # Repeat the sub-kernel across the 'sub-pixel' channels\n",
    "        # This makes all 4 pixels in a 2x2 block start identical\n",
    "        icnr_kernel = sub_kernel.repeat_interleave(upscale_factor**2, dim=0)\n",
    "        conv_layer.weight.data.copy_(icnr_kernel)\n",
    "\n",
    "    def forward(self, z, labels=None):\n",
    "        if self.conditional and labels is not None:\n",
    "            label_embed = self.label_embedding(labels)  # labels shape: [B]\n",
    "            z = z * label_embed\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "\n",
    "        x = F.relu(self.bn1(self.ps1(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.ps2(self.conv2(x))))\n",
    "        return torch.tanh(self.output_conv(x))\n",
    "\n",
    "\n",
    "# Instantiate and move to device\n",
    "g_model = Generator().to(device)\n",
    "print(g_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072ba5e",
   "metadata": {},
   "source": [
    "The conditional generator was implemented using a PixelShuffle-based upsampling architecture. Class conditioning was introduced through a learned embedding of the digit label into the latent space, combined multiplicatively with the noise vector to modulate the generation process. The model first projects the conditioned latent code into a 7Ã—7Ã—128 feature map, then performs two stages of sub-pixel upsampling (7â†’14â†’28) using convolution + PixelShuffle blocks with Batch Normalization and ReLU activations. To reduce checkerboard and dot artifacts that can emerge from upsampling, ICNR initialization was applied to the convolutional layers preceding PixelShuffle. Finally, the generator outputs a 28Ã—28 grayscale image through a Tanh activation, which is consistent with preprocessing that normalizes MNIST images to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856fd90",
   "metadata": {},
   "source": [
    "## 5.2 Conditional Discriminator (Standard & WGAN-GP Variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES, use_sigmoid=True, conditional=CONDITIONAL):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.conditional = conditional\n",
    "        \n",
    "        # Input channels: 1 for image, +1 for label channel if conditional\n",
    "        in_channels = 2 if conditional else 1\n",
    "\n",
    "        if conditional:\n",
    "            # Embed the class label into a vector of size 28*28\n",
    "            self.label_embedding = nn.Embedding(num_classes, 28 * 28)\n",
    "\n",
    "        # Main sequential network with Spectral Normalization\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is (in_channels, 28, 28): image channel + optional label channel\n",
    "            spectral_norm(nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Second conv block: (32, 14, 14) â†’ (64, 7, 7)\n",
    "            spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Flatten\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Dense layers with Spectral Normalization\n",
    "            spectral_norm(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            spectral_norm(nn.Linear(512, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels=None):\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        if self.conditional and labels is not None:\n",
    "            # Embed label and reshape to spatial map\n",
    "            label_embed = self.label_embedding(labels)  # labels shape: [B] -> [B, 784]\n",
    "            label_embed = label_embed.view(batch_size, 1, 28, 28)\n",
    "            # Concatenate image and label map\n",
    "            x = torch.cat([img, label_embed], dim=1)\n",
    "        else:\n",
    "            x = img\n",
    "\n",
    "        x = self.model(x)\n",
    "        \n",
    "        if self.use_sigmoid:\n",
    "            x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DiscriminatorWGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    WGAN-GP optimized Discriminator (Critic).\n",
    "    \n",
    "    Key differences from standard Discriminator:\n",
    "    - No Spectral Normalization (GP already enforces Lipschitz constraint)\n",
    "    - No Dropout (stochasticity interferes with gradient penalty calculation)\n",
    "    - LayerNorm for stability (no batch dependencies like BatchNorm)\n",
    "    - Larger capacity (64->128 filters) for better Wasserstein distance estimation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES, conditional=CONDITIONAL):\n",
    "        super().__init__()\n",
    "        self.conditional = conditional\n",
    "        \n",
    "        # Input channels: 1 for image, +1 for label channel if conditional\n",
    "        in_channels = 2 if conditional else 1\n",
    "\n",
    "        if conditional:\n",
    "            # Embed the class label into a vector of size 28*28\n",
    "            self.label_embedding = nn.Embedding(num_classes, 28 * 28)\n",
    "\n",
    "        # Main sequential network - no spectral norm, no dropout\n",
    "        # First conv block: (in_channels, 28, 28) -> (64, 14, 14)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([64, 14, 14])\n",
    "        \n",
    "        # Second conv block: (64, 14, 14) -> (128, 7, 7)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([128, 7, 7])\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(128 * 7 * 7, 512)\n",
    "        self.ln3 = nn.LayerNorm(512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, img, labels=None):\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        if self.conditional and labels is not None:\n",
    "            # Embed label and reshape to spatial map\n",
    "            label_embed = self.label_embedding(labels)  # [B] -> [B, 784]\n",
    "            label_embed = label_embed.view(batch_size, 1, 28, 28)\n",
    "            # Concatenate image and label map\n",
    "            x = torch.cat([img, label_embed], dim=1)\n",
    "        else:\n",
    "            x = img\n",
    "\n",
    "        # Conv blocks with LayerNorm\n",
    "        x = F.leaky_relu(self.ln1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.ln2(self.conv2(x)), 0.2)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.leaky_relu(self.ln3(self.fc1(x)), 0.2)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # No sigmoid - WGAN-GP always uses raw scores\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d1bce",
   "metadata": {},
   "source": [
    "**Conditional Discriminator Architecture**\n",
    "\n",
    "The conditional discriminator was implemented using a label-map concatenation strategy, where each class label is embedded into a 28Ã—28 spatial map and concatenated with the input image as an additional channel. This formulation allows the discriminator to jointly assess image realism and class consistency by conditioning its decision on both visual content and the target label.\n",
    "\n",
    "For the standard adversarial objectives (BCE, LSGAN, and hinge), the discriminator architecture consists of two convolutional blocks that progressively downsample the input resolution (28Ã—28 â†’ 14Ã—14 â†’ 7Ã—7), followed by fully connected layers that output a single authenticity score. Spectral Normalization (SN) is applied to both convolutional and linear layers to constrain the Lipschitz constant of the discriminator, which stabilizes adversarial training by reducing gradient explosions, oscillatory dynamics, and susceptibility to mode collapse. Dropout is included as a regularization mechanism to improve generalization and mitigate overfitting in the discriminator. Depending on the selected loss function, the final output is either passed through a sigmoid (for BCE) or left as raw logits (for LSGAN and hinge), enabling each objective to apply its appropriate formulation.\n",
    "\n",
    "**WGAN-GP Critic Variant (DiscriminatorWGAN)**\n",
    "\n",
    "For WGAN-GP, a dedicated critic architecture is employed, reflecting the methodological requirements of Wasserstein-based training. In this variant, Spectral Normalization and Dropout are explicitly removed and replaced with Layer Normalization. This design choice is motivated by three considerations:\n",
    "\n",
    "1. Lipschitz regularization: Spectral Normalization and gradient penalty both enforce Lipschitz constraints; combining them results in redundant regularization and can over-constrain the critic, degrading the quality of the learned Wasserstein distance.\n",
    "\n",
    "2. Gradient penalty stability: Dropout introduces stochastic perturbations in the criticâ€™s forward pass, which interferes with the computation of reliable input gradients required by the gradient penalty term.\n",
    "\n",
    "3. Critic capacity: The channel width is increased (from 64 to 128 filters in deeper layers) to provide sufficient representational capacity for accurately approximating the Wasserstein distance between real and generated distributions.\n",
    "\n",
    "The WGAN-GP critic outputs raw scalar scores without any sigmoid activation, as required by the Wasserstein formulation. The appropriate discriminator variant (standard SN-based discriminator vs. WGAN-GP critic) is selected automatically based on the chosen adversarial objective, ensuring that each loss strategy is paired with an architecture consistent with its theoretical assumptions and practical stability requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db53b",
   "metadata": {},
   "source": [
    "## 5.3 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    DCGAN-style weight initialization.\n",
    "    Skip spectral_norm wrapped layers (they handle their own init).\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 and not hasattr(m, 'weight_orig'):\n",
    "        # Conv layer without spectral norm\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1 and not hasattr(m, 'weight_orig'):\n",
    "        # Linear layer without spectral norm\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc3f8d",
   "metadata": {},
   "source": [
    "Model parameters were initialized following the DCGAN guidelines, which recommend sampling convolutional and fully connected weights from a Normal(0, 0.02) distribution and initializing batch normalization layers with weights drawn from Normal(1, 0.02) and zero bias. This initialization scheme has been shown to promote stable adversarial dynamics during the early stages of training and to reduce the likelihood of vanishing gradients or premature mode collapse.\n",
    "\n",
    "Label embedding layers were initialized using the same Normal(0, 0.02) distribution to ensure comparable scale between latent and conditional representations. For layers wrapped with Spectral Normalization, initialization was applied to the underlying unnormalized weight parameters (weight_orig) rather than the normalized weights, ensuring consistent parameter scaling at initialization while preserving the Lipschitz constraint enforced during training. This design choice aligns with best practices in stabilizing GAN training, as discussed in the DCGAN and SNGAN literature, where careful weight initialization and spectral normalization jointly contribute to improved convergence behavior and reduced training oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec7284",
   "metadata": {},
   "source": [
    "# 6. Training & Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e30af",
   "metadata": {},
   "source": [
    "## 6.1 Class Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a3795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures % of generated images where classifier prediction matches conditioned label.\n",
    "# Uses the same calibrated classifier from the game for consistency.\n",
    "\n",
    "# Classifier architecture (must match checkpoint)\n",
    "class MNISTCNNCalibrated(nn.Module):\n",
    "    \"\"\"Calibrated CNN for MNIST - architecture only, no Lightning dependencies.\"\"\"\n",
    "    \n",
    "    def __init__(self, width=128, depth=3, dropout_p=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: depth blocks of Conv(3x3) -> ReLU -> MaxPool(2)\n",
    "        channels = [width, 2 * width, 2 * width][:depth]\n",
    "        in_ch = 1\n",
    "        blocks = []\n",
    "        for out_ch in channels:\n",
    "            blocks.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True))\n",
    "            blocks.append(nn.ReLU())\n",
    "            blocks.append(nn.MaxPool2d(kernel_size=2))\n",
    "            in_ch = out_ch\n",
    "        self.encoder = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Head: flatten -> FC(width) -> ReLU -> dropout -> FC(num_classes)\n",
    "        spatial = 28 // (2 ** depth)\n",
    "        feat_dim = channels[-1] * spatial * spatial\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feat_dim, width),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(width, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(self.encoder(x))\n",
    "\n",
    "\n",
    "def load_calibrated_classifier(checkpoint_path):\n",
    "    \"\"\"Load the calibrated classifier from checkpoint.\"\"\"\n",
    "    classifier = MNISTCNNCalibrated(width=128, depth=3, dropout_p=0.1)\n",
    "    \n",
    "    # Load checkpoint (Lightning format)\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    # Remove 'model.' prefix if present (Lightning adds this sometimes)\n",
    "    clean_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        clean_key = k.replace('model.', '') if k.startswith('model.') else k\n",
    "        clean_state[clean_key] = v\n",
    "    \n",
    "    classifier.load_state_dict(clean_state)\n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def evaluate_class_consistency(g_model, classifier, samples_per_class=500):\n",
    "    \"\"\"\n",
    "    Evaluate class-consistency accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        overall_accuracy: % of generated images where prediction matches conditioned label\n",
    "        per_class_accuracy: List of accuracies for each digit (0-9)\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    g_model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    per_class_correct = [0] * 10\n",
    "    per_class_total = [0] * 10\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            labels = torch.full((samples_per_class,), digit, device=device).long()\n",
    "            noise = torch.randn(samples_per_class, LATENT_DIM, device=device)\n",
    "            \n",
    "            # Generate images (output range: [-1, 1])\n",
    "            fake_imgs = g_model(noise, labels)\n",
    "            \n",
    "            # Convert to classifier input range [0, 1]\n",
    "            fake_imgs_normalized = (fake_imgs + 1) / 2\n",
    "            \n",
    "            # Classify\n",
    "            logits = classifier(fake_imgs_normalized)\n",
    "            predicted = logits.argmax(dim=1)\n",
    "            \n",
    "            # Count matches\n",
    "            matches = (predicted == labels).sum().item()\n",
    "            correct += matches\n",
    "            total += samples_per_class\n",
    "            per_class_correct[digit] = matches\n",
    "            per_class_total[digit] = samples_per_class\n",
    "    \n",
    "    overall_accuracy = 100.0 * correct / total\n",
    "    per_class_accuracy = [100.0 * per_class_correct[i] / per_class_total[i] for i in range(10)]\n",
    "    \n",
    "    return overall_accuracy, per_class_accuracy\n",
    "\n",
    "\n",
    "# Load classifier\n",
    "print(\"Loading calibrated classifier for class-consistency evaluation...\")\n",
    "try:\n",
    "    mnist_classifier = load_calibrated_classifier(CLASSIFIER_CHECKPOINT)\n",
    "    print(f\"Classifier loaded from: {CLASSIFIER_CHECKPOINT}\")\n",
    "    \n",
    "    # Verify classifier accuracy on real MNIST\n",
    "    classifier_correct = 0\n",
    "    classifier_total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in train_loader:\n",
    "            # Note: train_loader images are [-1, 1], convert to [0, 1]\n",
    "            imgs = ((imgs + 1) / 2).to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = mnist_classifier(imgs)\n",
    "            predicted = logits.argmax(dim=1)\n",
    "            classifier_total += labels.size(0)\n",
    "            classifier_correct += (predicted == labels).sum().item()\n",
    "    print(f\"Classifier accuracy on real MNIST: {100.0 * classifier_correct / classifier_total:.2f}%\")\n",
    "    CLASSIFIER_AVAILABLE = True\n",
    "except FileNotFoundError:\n",
    "    print(f\"WARNING: Classifier checkpoint not found at {CLASSIFIER_CHECKPOINT}\")\n",
    "    print(\"Class-consistency evaluation will be skipped.\")\n",
    "    mnist_classifier = None\n",
    "    CLASSIFIER_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ed24c",
   "metadata": {},
   "source": [
    "**Model Instantiation, Loss Strategy Selection**\n",
    "\n",
    "Models were instantiated on the selected compute device, with the generator using its internal initialization (including ICNR initialization for PixelShuffle upsampling) and the discriminator initialized using DCGAN-style weight initialization to improve early training stability. A modular loss-strategy interface was used to select the adversarial objective (BCE, LSGAN, hinge, or WGAN-GP). For WGAN-GP, the discriminator instance was passed into the strategy object to enable computation of the gradient penalty term, enforcing the Lipschitz constraint required by the Wasserstein formulation.\n",
    "\n",
    "Training used the Two Time-Scale Update Rule (TTUR), configuring separate Adam optimizers for discriminator and generator with a higher discriminator learning rate (LR_D > LR_G) and GAN-optimized momentum parameters (Î²â‚=0.5, Î²â‚‚=0.999). This setup supports stable adversarial dynamics and enables fair benchmarking across different loss formulations under consistent optimization conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e8547",
   "metadata": {},
   "source": [
    "**Class-Consistency Evaluation (Label-Alignment Metric)**\n",
    "\n",
    "To complement distribution-based metrics (FID/KID) and qualitative inspection, we introduced a class-consistency evaluation to explicitly measure whether the conditional generator obeys the requested label. This metric quantifies label alignment by computing the percentage of generated images whose predicted class (from an external classifier) matches the conditioning label ð‘¦. This is particularly relevant for conditional GANs on MNIST because a model can produce visually plausible digits while still ignoring conditioning information (a known failure mode in cGANs).\n",
    "\n",
    "**Metric definition**\n",
    "\n",
    "- For each class ð‘¦âˆˆ{0,â€¦,9}, we generate ð‘ samples using ðº(ð‘§,ð‘¦), then evaluate them with a pretrained MNIST classifier ð¶(â‹…). \n",
    "\n",
    "We also report per-class consistency, which is useful to detect asymmetric failures (e.g., digits that collapse into visually similar classes such as 3/5/8 or 4/9):\n",
    "\n",
    "- *Directly targets the conditioning objective: Unlike FID/KID, this metric evaluates whether the generator respects the provided label.*\n",
    "- *Detects â€œconditional collapseâ€: The generator may learn a narrow subset of digits and still appear stable, but class-consistency immediately exposes label leakage or label ignoring.*\n",
    "- *MNIST-appropriate: A digit classifier is trained on the same domain (handwritten digits), making it a strong and interpretable control metric.*\n",
    "\n",
    "**Classifier choice and calibration**\n",
    "\n",
    "We used the same MNIST classifier employed previously (in our previous MNIST project) to ensure consistency across experiments. The classifier architecture (a custom CNN) was re-instantiated without Lightning dependencies, and the weights were loaded from a Lightning-style checkpoint (state_dict).\n",
    "\n",
    "Before using the classifier for evaluation, we verified that it achieved high accuracy on real MNIST to ensure it is a reliable oracle for class-consistency. If the checkpoint is missing, the evaluation is safely skipped (to avoid breaking the benchmark pipeline).\n",
    "\n",
    "This evaluation was applied to each trained generator configuration (different loss strategies or regularization setups), enabling a clear comparison of how each strategy affects label compliance, not only visual fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd3a42",
   "metadata": {},
   "source": [
    "## 6.2 Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a066dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark training loop\n",
    "\n",
    "# Store results\n",
    "benchmark_results = {}\n",
    "\n",
    "def run_benchmark(strategies=BENCHMARK_STRATEGIES, num_steps=NUM_STEPS, save_interval=SAVE_INTERVAL):\n",
    "    \"\"\"\n",
    "    Run training for each loss strategy and collect metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Emit benchmark start\n",
    "    if LIVE_MONITOR:\n",
    "        emit_benchmark_start(strategies, num_steps)\n",
    "    \n",
    "    for strategy_idx, strategy_name in enumerate(strategies):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING: {strategy_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Emit strategy start\n",
    "        if LIVE_MONITOR:\n",
    "            emit_strategy_start(strategy_name, strategy_idx, len(strategies))\n",
    "        \n",
    "        # Reset random seeds for fair comparison\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        # Instantiate fresh models using factories for strategy-specific configs\n",
    "        g_model = Generator().to(device)\n",
    "        loss_strategy = get_loss_strategy(strategy_name, device, lambda_gp=LAMBDA_GP)\n",
    "        d_model = get_discriminator(strategy_name, device)\n",
    "        d_model.apply(weights_init)\n",
    "        loss_strategy.set_d_model(d_model)\n",
    "        \n",
    "        # Optimizers with strategy-specific betas\n",
    "        adam_betas = get_adam_betas(strategy_name)\n",
    "        optimizer_d = optim.Adam(d_model.parameters(), lr=LR_D, betas=adam_betas)\n",
    "        optimizer_g = optim.Adam(g_model.parameters(), lr=LR_G, betas=adam_betas)\n",
    "        \n",
    "        # Training state\n",
    "        losses = {\"G\": [], \"D\": []}\n",
    "        data_iter = iter(train_loader)\n",
    "        n_critic = loss_strategy.n_critic\n",
    "        d_loss = torch.tensor(0.0)\n",
    "        \n",
    "        # Fixed test samples for visualization\n",
    "        samples_test = torch.randn(16, LATENT_DIM, device=device)\n",
    "        # Balanced labels for monitoring (0-9, 0-5) ensures all digits represented\n",
    "        labels_test = torch.arange(0, 10, device=device).repeat(2)[:16].long()  # shape: [B]\n",
    "        \n",
    "        # Timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # --- Train Discriminator ---\n",
    "            for _ in range(n_critic):\n",
    "                try:\n",
    "                    real_imgs, batch_labels = next(data_iter)\n",
    "                except StopIteration:\n",
    "                    data_iter = iter(train_loader)\n",
    "                    real_imgs, batch_labels = next(data_iter)\n",
    "\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                batch_labels = batch_labels.to(device).long()  # shape: [B]\n",
    "\n",
    "                noise = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "                # Pass labels only if conditional mode\n",
    "                labels_for_g = batch_labels if CONDITIONAL else None\n",
    "                fake_imgs = g_model(noise, labels_for_g)\n",
    "\n",
    "                optimizer_d.zero_grad()\n",
    "                d_real_out = d_model(real_imgs, labels_for_g)\n",
    "                d_loss_real = loss_strategy.d_loss_real(d_real_out)\n",
    "                d_fake_out = d_model(fake_imgs.detach(), labels_for_g)\n",
    "                d_loss_fake = loss_strategy.d_loss_fake(d_fake_out)\n",
    "                gp = loss_strategy.gradient_penalty(real_imgs, fake_imgs.detach(), labels_for_g)\n",
    "                d_loss = loss_strategy.compute_d_loss(d_loss_real, d_loss_fake, gp)\n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            optimizer_g.zero_grad()\n",
    "            z = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "            if CONDITIONAL:\n",
    "                gen_labels = torch.randint(0, 10, (BATCH_SIZE,), device=device).long()  # shape: [B]\n",
    "            else:\n",
    "                gen_labels = None\n",
    "            gen_imgs = g_model(z, gen_labels)\n",
    "            g_out = d_model(gen_imgs, gen_labels)\n",
    "            g_loss = loss_strategy.g_loss(g_out)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            losses[\"G\"].append(g_loss.item())\n",
    "            losses[\"D\"].append(d_loss.item())\n",
    "\n",
    "            if step % save_interval == 0:\n",
    "                print(f\"Step {step} â€” D: {d_loss.item():.4f}, G: {g_loss.item():.4f}\")\n",
    "\n",
    "            # Live monitor\n",
    "            if LIVE_MONITOR and step % EMIT_INTERVAL == 0:\n",
    "                with torch.no_grad():\n",
    "                    monitor_labels = labels_test if CONDITIONAL else None\n",
    "                    monitor_samples = g_model(samples_test, monitor_labels)\n",
    "                emit_frames(monitor_samples, labels_test, step, g_loss.item(), d_loss.item(), num_steps)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\nEvaluating {strategy_name}...\")\n",
    "        # Class-consistency only makes sense for conditional models\n",
    "        classifier_for_eval = mnist_classifier if (CLASSIFIER_AVAILABLE and CONDITIONAL) else None\n",
    "        fid_score, kid_mean, kid_std, class_acc, per_class_acc = evaluate_model_for_benchmark(g_model, classifier_for_eval)\n",
    "        \n",
    "        # Emit strategy end with results\n",
    "        if LIVE_MONITOR:\n",
    "            emit_strategy_end(strategy_name, fid_score, kid_mean, kid_std, training_time)\n",
    "        \n",
    "        # Generate final samples\n",
    "        with torch.no_grad():\n",
    "            final_labels = labels_test if CONDITIONAL else None\n",
    "            final_samples = g_model(samples_test, final_labels)\n",
    "        \n",
    "        # Store results (including optimizer states for resumable training)\n",
    "        results[strategy_name] = {\n",
    "            \"losses\": losses,\n",
    "            \"fid\": fid_score,\n",
    "            \"kid_mean\": kid_mean,\n",
    "            \"kid_std\": kid_std,\n",
    "            \"class_consistency\": class_acc,\n",
    "            \"per_class_consistency\": per_class_acc,\n",
    "            \"training_time\": training_time,\n",
    "            \"step\": num_steps,\n",
    "            \"g_model_state\": deepcopy(g_model.state_dict()),\n",
    "            \"d_model_state\": deepcopy(d_model.state_dict()),\n",
    "            \"g_optimizer_state\": deepcopy(optimizer_g.state_dict()),\n",
    "            \"d_optimizer_state\": deepcopy(optimizer_d.state_dict()),\n",
    "            \"final_samples\": final_samples.cpu(),\n",
    "            \"labels_test\": labels_test.cpu(),\n",
    "            \"hyperparameters\": {\n",
    "                \"strategy\": strategy_name,\n",
    "                \"latent_dim\": LATENT_DIM,\n",
    "                \"num_classes\": NUM_CLASSES,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"lr_g\": LR_G,\n",
    "                \"lr_d\": LR_D,\n",
    "                \"adam_betas\": adam_betas,\n",
    "                \"n_critic\": n_critic,\n",
    "                \"conditional\": CONDITIONAL,\n",
    "            },\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{strategy_name.upper()} Results:\")\n",
    "        print(f\"  FID: {fid_score:.2f}\")\n",
    "        print(f\"  KID: {kid_mean:.4f} Â± {kid_std:.4f}\")\n",
    "        if class_acc is not None:\n",
    "            print(f\"  Class-Consistency: {class_acc:.2f}%\")\n",
    "        print(f\"  Time: {training_time:.1f}s\")\n",
    "    \n",
    "    # Emit benchmark complete\n",
    "    if LIVE_MONITOR:\n",
    "        emit_done()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_for_benchmark(g_model, classifier=None):\n",
    "    \"\"\"Evaluation function for benchmarking (returns values, doesn't print).\"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "    kid = KernelInceptionDistance(feature=2048, subset_size=100).to(device)\n",
    "\n",
    "    idx = np.random.randint(0, len(train_dataset), NUM_EVAL_SAMPLES)\n",
    "    batch_size = 256\n",
    "    \n",
    "    for i in range(0, NUM_EVAL_SAMPLES, batch_size):\n",
    "        batch_idx = idx[i:i + batch_size]\n",
    "        real_batch = torch.stack([train_dataset[j][0] for j in batch_idx])\n",
    "        real_batch = ((real_batch + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "        real_batch = real_batch.repeat(1, 3, 1, 1)\n",
    "        real_batch = torch.nn.functional.interpolate(\n",
    "            real_batch.float(), size=(299, 299), mode='bilinear', align_corners=False\n",
    "        ).to(torch.uint8).to(device)\n",
    "        fid.update(real_batch, real=True)\n",
    "        kid.update(real_batch, real=True)\n",
    "\n",
    "    for i in range(0, NUM_EVAL_SAMPLES, batch_size):\n",
    "        current_batch = min(batch_size, NUM_EVAL_SAMPLES - i)\n",
    "        noise = torch.randn(current_batch, LATENT_DIM, device=device)\n",
    "        if CONDITIONAL:\n",
    "            labels = torch.randint(0, 10, (current_batch,), device=device).long()  # shape: [B]\n",
    "        else:\n",
    "            labels = None\n",
    "        with torch.no_grad():\n",
    "            fake_batch = g_model(noise, labels)\n",
    "        fake_batch = ((fake_batch + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "        fake_batch = fake_batch.repeat(1, 3, 1, 1)\n",
    "        fake_batch = torch.nn.functional.interpolate(\n",
    "            fake_batch.float(), size=(299, 299), mode='bilinear', align_corners=False\n",
    "        ).to(torch.uint8).to(device)\n",
    "        fid.update(fake_batch, real=False)\n",
    "        kid.update(fake_batch, real=False)\n",
    "\n",
    "    fid_score = fid.compute().item()\n",
    "    kid_mean, kid_std = kid.compute()\n",
    "    \n",
    "    # Class-consistency evaluation\n",
    "    if classifier is not None:\n",
    "        class_acc, per_class_acc = evaluate_class_consistency(g_model, classifier)\n",
    "    else:\n",
    "        class_acc, per_class_acc = None, None\n",
    "    \n",
    "    return fid_score, kid_mean.item(), kid_std.item(), class_acc, per_class_acc\n",
    "\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_results = run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241108e8",
   "metadata": {},
   "source": [
    "## 6.3 Comparative Analysis of Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923fdc9",
   "metadata": {},
   "source": [
    "Under a controlled setup (same dataset, generator/discriminator family, TTUR, and step budget), the benchmark compared four adversarial objectives using three evaluation axes: distributional fidelity (FID, KID), conditional correctness (Class-Consistency), and computational cost (training time).\n",
    "\n",
    "**Overall ranking**\n",
    "\n",
    "- **WGAN-GP** achieved the best quantitative performance: FID = 4.79, KID = 0.0028 Â± 0.0015, with the highest Class-Consistency = 98.90%, but it was also the slowest (1857s).\n",
    "- **BCE** and **LSGAN** formed a strong middle tier with similar class-consistency and runtime, achieving competitive fidelity:\n",
    "  - BCE: FID = 5.06, KID = 0.0024 Â± 0.0020, Class-Consistency = 97.90%, 495s\n",
    "  - LSGAN: FID = 5.22, KID = 0.0028 Â± 0.0020, Class-Consistency = 98.46%, 513s\n",
    "- **Hinge** underperformed in fidelity while keeping class-consistency comparable:\n",
    "  - Hinge: FID = 6.84, KID = 0.0033 Â± 0.0021, Class-Consistency = 97.80%, 502s\n",
    "\n",
    "**What the numbers suggest**\n",
    "\n",
    "1. **Fidelity (FID/KID): WGAN-GP leads, but BCE/LSGAN are close**\n",
    "\n",
    "   WGAN-GP's FID 4.79 and KID 0.0028 indicate the closest alignment between generated and real sample distributions among all strategies tested. This is consistent with the Wasserstein objective, which provides smoother, more informative gradients than probability-based discriminators, and with the gradient penalty, which encourages a well-behaved critic function.\n",
    "\n",
    "   Notably, BCE and LSGAN achieve strong results (FID 5.06 and 5.22 respectively), narrowing the gap to WGAN-GP compared to earlier configurations. BCE achieves the lowest KID (0.0024), indicating excellent second-order distributional alignment. Hinge lags behind at FID 6.84, suggesting that in this architecture/regime, margin-based optimization did not translate into improved distributional match.\n",
    "\n",
    "2. **Conditional correctness: all strategies achieve high consistency**\n",
    "\n",
    "   Class-consistency is high across all objectives (97.80%â€“98.90%), meaning that conditioning works reliably: the generator typically produces a digit that matches the requested label.\n",
    "\n",
    "   WGAN-GP achieves the highest consistency (98.90%), followed closely by LSGAN (98.46%), BCE (97.90%), and Hinge (97.80%). The uniformly high scores indicate that the conditioning mechanism is effective regardless of the adversarial objective, with WGAN-GP providing marginally tighter coupling between label-conditioning and generated structure.\n",
    "\n",
    "3. **Efficiency: WGAN-GP is ~3.7Ã— slower**\n",
    "\n",
    "   WGAN-GP took 1857s, versus ~495â€“513s for the other losses. This is fully expected because:\n",
    "\n",
    "   - it uses multiple critic updates per generator update (*n_critic* = 5), and\n",
    "   - computes gradient penalty, which requires additional forward and backward passes through the critic on interpolated samples.\n",
    "\n",
    "  So WGAN-GP provides the best quality here, but at a significant computational cost.\n",
    "\n",
    "**Interpreting the loss curves**  \n",
    "\n",
    "Loss magnitudes are not directly comparable across objectives because the losses represent different quantities:\n",
    "\n",
    "- **BCE / LSGAN** losses are tied to probabilistic classification / regression-to-targets (their absolute values remain in relatively bounded ranges). BCE stabilizes around 0.8, LSGAN around 0.33.\n",
    "- **Hinge** uses margin penalties; its generator loss oscillates around zero throughout training, reflecting the margin-based formulation.\n",
    "- **WGAN-GP** losses represent critic score differences plus penalty; the generator loss becoming strongly negative (reaching approximately -34) is not inherently \"bad\"â€”it indicates the critic assigns high scores to generated samples and must be interpreted alongside FID/KID and visual samples.\n",
    "\n",
    "Given that WGAN-GP achieves the best FID/KID and class-consistency, its loss trajectory is consistent with effective training rather than instability.\n",
    "\n",
    "**Key Takeaways**\n",
    "\n",
    "1. **Best quality**: WGAN-GP achieved the strongest fidelity and conditional correctness (FID 4.79, KID 0.0028, 98.90% consistency).\n",
    "2. **Best speed/quality trade-off**: BCE offers excellent results (FID 5.06, 97.90% consistency) with the fastest training time (495s). LSGAN performs comparably (FID 5.22, 98.46% consistency, 513s).\n",
    "3. **Hinge not ideal here**: Hinge produced worse fidelity (FID 6.84) without improving conditional correctness, suggesting it may require different regularization/capacity or a different discriminator configuration to be competitive in this setup.\n",
    "4. **Compute matters**: WGAN-GP's gains come at ~3.7Ã— training time, which is important when selecting a practical default strategy for resource-constrained scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc049e",
   "metadata": {},
   "source": [
    "## 6.4 Benchmarking Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary table\n",
    "\n",
    "def print_results_table(results):\n",
    "    \"\"\"Print a formatted comparison table.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Strategy':<12} {'FID':>10} {'KID':>18} {'Time (s)':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Sort by FID (best first)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['fid'])\n",
    "    \n",
    "    for i, (name, data) in enumerate(sorted_results):\n",
    "        rank = \"ðŸ¥‡\" if i == 0 else \"ðŸ¥ˆ\" if i == 1 else \"ðŸ¥‰\" if i == 2 else \"  \"\n",
    "        kid_str = f\"{data['kid_mean']:.4f} Â± {data['kid_std']:.4f}\"\n",
    "        print(f\"{rank} {name:<10} {data['fid']:>10.2f} {kid_str:>18} {data['training_time']:>12.1f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Best performer\n",
    "    best = sorted_results[0]\n",
    "    print(f\"\\nâœ“ Best performer: {best[0].upper()} (FID: {best[1]['fid']:.2f})\")\n",
    "\n",
    "print_results_table(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b53b28",
   "metadata": {},
   "source": [
    "### 6.4.1 Comparative Analysis of Adversarial Loss Strategies\n",
    "\n",
    "This section provides a critical comparison of four adversarial objectives: **BCE**, **LSGAN**, **Hinge**, and **WGAN-GP**, grounded both in the empirical benchmark results and in the theoretical motivations discussed in the course materials (GAN objectives, stability considerations, and troubleshooting guidelines).\n",
    "\n",
    "#### Quantitative Summary\n",
    "\n",
    "The benchmark results indicate the following ranking in terms of distributional fidelity (FID, KID):\n",
    "\n",
    "- **WGAN-GP**: FID = **4.79**, KID = **0.0028 Â± 0.0015**, Class-Consistency = **98.90%**, Time = **1857s**  \n",
    "- **BCE**: FID = **5.06**, KID = **0.0024 Â± 0.0020**, Class-Consistency = **97.90%**, Time = **495s**  \n",
    "- **LSGAN**: FID = **5.22**, KID = **0.0028 Â± 0.0020**, Class-Consistency = **98.46%**, Time = **513s**  \n",
    "- **Hinge**: FID = **6.84**, KID = **0.0033 Â± 0.0021**, Class-Consistency = **97.80%**, Time = **502s**\n",
    "\n",
    "Lower FID and KID values correspond to a closer match between the generated and real data distributions. Training time reflects the practical computational cost associated with each objective.\n",
    "\n",
    "#### 6.4.2 Theoretical Interpretation and Practical Implications\n",
    "\n",
    "**WGAN-GP** achieved the strongest performance in terms of FID and class-consistency, which is consistent with the theoretical foundations of the Wasserstein distance. As discussed in the lecture materials on GAN stability and loss functions, Wasserstein objectives provide smoother gradients even when the discriminator (critic) is near optimal, alleviating vanishing gradient issues that commonly arise in the original BCE-based GAN formulation. The inclusion of a **gradient penalty** further enforces the Lipschitz constraint required for the Wasserstein formulation, leading to more stable optimization dynamics and improved sample fidelity.\n",
    "\n",
    "However, these benefits come at a substantial computational cost: WGAN-GP required approximately **3.7Ã— longer training time** than the other strategies due to (i) multiple critic updates per generator step (*n_critic* = 5) and (ii) the additional backward passes needed to compute the gradient penalty. This confirms the trade-off highlighted in the course notes: while WGAN-GP is often more stable and produces higher-quality samples, it is significantly more expensive to train and may be impractical in resource-constrained settings.\n",
    "\n",
    "**BCE** and **LSGAN** formed a strong middle tier, achieving very similar FID scores (5.06 and 5.22 respectively) and class-consistency (~98%), while requiring only about one-quarter of WGAN-GP's training time. Notably, **BCE achieved the lowest KID (0.0024)**, suggesting excellent second-order distributional alignment. The competitive performance of both objectives aligns with the motivation presented in the lectures: when combined with modern stabilization techniques (TTUR, Spectral Normalization, conditional inputs), classical GAN losses remain highly effective for low-resolution conditional generation tasks.\n",
    "\n",
    "**Hinge loss**, commonly adopted in large-scale architectures such as SAGAN and BigGAN, underperformed in this benchmark. Despite achieving class-consistency comparable to BCE (97.80%), Hinge exhibited noticeably worse FID (6.84) and KID (0.0033). This outcome suggests that margin-based objectives may require either higher model capacity, stronger regularization, or larger and more complex datasets to fully realize their advantages. In the present low-resolution MNIST setting, the hinge formulation did not translate into improved distributional alignment, echoing the course discussion that no single GAN loss is universally optimal across domains and scales.\n",
    "\n",
    "#### 6.4.3 Stability Considerations and Architectureâ€“Loss Interactions\n",
    "\n",
    "The observed performance differences must also be interpreted in light of the discriminator design choices discussed in the lectures on GAN stabilization. In particular, the interaction between **Spectral Normalization, gradient penalty, and regularization mechanisms (e.g., Dropout or Layer Normalization)** can significantly influence training dynamics.\n",
    "\n",
    "A key architectural decision in this implementation was the use of a **dedicated `DiscriminatorWGAN` class** for the WGAN-GP objective. This variant removes Spectral Normalization and Dropout (which conflict with gradient penalty computation) and replaces them with LayerNorm, while also increasing model capacity (64â†’128 filters). The strong performance of WGAN-GP in this configuration validates the theoretical claim that explicitly enforcing Lipschitz continuity via gradient penalty requires careful architectural adaptation to avoid over-regularization.\n",
    "\n",
    "Conversely, the competitive performance of BCE and LSGAN highlights that simpler objectives, when combined with appropriate architectural and optimization heuristics (e.g., TTUR, Spectral Normalization, careful initialization), can achieve stable training and high-quality results at a fraction of the computational cost.\n",
    "\n",
    "#### 6.4.4 Overall Assessment\n",
    "\n",
    "The benchmark confirms several key points emphasized in the course materials:\n",
    "\n",
    "- **There is a clear qualityâ€“efficiency trade-off**: WGAN-GP delivers the highest sample fidelity (FID 4.79) and class-consistency (98.90%) but at significantly higher computational cost (3.7Ã— longer training).\n",
    "- **BCE and LSGAN remain strong practical baselines**, offering a favorable balance between training stability, sample quality, and efficiency. BCE in particular achieves excellent results (FID 5.06) with the fastest training time.\n",
    "- **Hinge loss is not universally superior** and appears less suited to low-resolution conditional MNIST generation under the present architectural constraints.\n",
    "- **Architectureâ€“objective alignment matters**: WGAN-GP's strong performance depends critically on using an appropriate discriminator design that avoids conflicting regularization mechanisms.\n",
    "\n",
    "These findings reinforce the importance of selecting the adversarial objective not only based on theoretical guarantees, but also considering dataset complexity, model capacity, architectural compatibility, and available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5470cae",
   "metadata": {},
   "source": [
    "# 7. Results Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization\n",
    "\n",
    "def plot_benchmark_comparison(results):\n",
    "    \"\"\"Create comprehensive comparison visualizations.\"\"\"\n",
    "    strategies = list(results.keys())\n",
    "    n_strategies = len(strategies)\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = {'bce': '#3498db', 'lsgan': '#2ecc71', 'hinge': '#e74c3c', 'wgan-gp': '#9b59b6'}\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # --- 1. FID Bar Chart (top left) ---\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    x = np.arange(n_strategies)\n",
    "    width = 0.35\n",
    "    \n",
    "    fids = [results[s]['fid'] for s in strategies]\n",
    "    \n",
    "    bars1 = ax1.bar(x, fids, width*1.5, color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax1.set_ylabel('FID (lower is better)')\n",
    "    ax1.set_title('FID Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([s.upper() for s in strategies])\n",
    "    ax1.axhline(y=min(fids), color='green', linestyle='--', alpha=0.5, label=f'Best: {min(fids):.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    for bar, val in zip(bars1, fids):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # --- 2. KID Bar Chart (top middle) ---\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    kid_means = [results[s]['kid_mean'] for s in strategies]\n",
    "    kid_stds = [results[s]['kid_std'] for s in strategies]\n",
    "    \n",
    "    bars2 = ax2.bar(x, kid_means, width*1.5, yerr=kid_stds, capsize=5,\n",
    "                    color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax2.set_ylabel('KID (lower is better)')\n",
    "    ax2.set_title('KID Comparison (with std)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([s.upper() for s in strategies])\n",
    "    \n",
    "    # --- 3. Training Time (top right) ---\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    times = [results[s]['training_time'] for s in strategies]\n",
    "    bars3 = ax3.bar(x, times, width*1.5, color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.set_title('Training Time')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([s.upper() for s in strategies])\n",
    "    \n",
    "    for bar, val in zip(bars3, times):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{val:.0f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # --- 4. Loss Curves with Dual Y-Axis (bottom left, spans 2 columns) ---\n",
    "    ax4 = fig.add_subplot(2, 3, (4, 5))\n",
    "    \n",
    "    # Smoothing function\n",
    "    def smooth(data, window=100):\n",
    "        if len(data) > window:\n",
    "            return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        return data\n",
    "    \n",
    "    # Plot BCE, LSGAN, Hinge on left axis\n",
    "    for s in ['bce', 'lsgan', 'hinge']:\n",
    "        if s in results:\n",
    "            losses_g = results[s]['losses']['G']\n",
    "            smoothed = smooth(losses_g)\n",
    "            ax4.plot(smoothed, label=f'{s.upper()} (G)', color=colors[s], linewidth=1.5)\n",
    "    \n",
    "    ax4.set_xlabel('Step')\n",
    "    ax4.set_ylabel('Generator Loss (BCE / LSGAN / Hinge)')\n",
    "    ax4.set_ylim(-0.5, 1.5)\n",
    "    ax4.legend(loc='upper left')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_title('Generator Loss Curves')\n",
    "    \n",
    "    # Plot WGAN-GP on right axis\n",
    "    if 'wgan-gp' in results:\n",
    "        ax4_right = ax4.twinx()\n",
    "        losses_g = results['wgan-gp']['losses']['G']\n",
    "        smoothed = smooth(losses_g)\n",
    "        ax4_right.plot(smoothed, label='WGAN-GP (G)', color=colors['wgan-gp'], \n",
    "                       linewidth=1.5, linestyle='--')\n",
    "        ax4_right.set_ylabel('Generator Loss (WGAN-GP)', color=colors['wgan-gp'])\n",
    "        ax4_right.tick_params(axis='y', labelcolor=colors['wgan-gp'])\n",
    "        ax4_right.legend(loc='upper right')\n",
    "    \n",
    "    # --- 5. Radar/Spider Chart (bottom right) ---\n",
    "    ax5 = fig.add_subplot(2, 3, 6, projection='polar')\n",
    "    \n",
    "    max_fid = max(fids)\n",
    "    max_kid = max(kid_means)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    # Check if class-consistency is available\n",
    "    has_class_consistency = all(results[s].get('class_consistency') is not None for s in strategies)\n",
    "    \n",
    "    if has_class_consistency:\n",
    "        metrics = ['FID\\nQuality', 'KID\\nQuality', 'Class\\nConsistency', 'Speed']\n",
    "    else:\n",
    "        metrics = ['FID\\nQuality', 'KID\\nQuality', 'Speed']\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    for s in strategies:\n",
    "        values = [\n",
    "            1 - results[s]['fid'] / max_fid,\n",
    "            1 - results[s]['kid_mean'] / max_kid,\n",
    "        ]\n",
    "        if has_class_consistency:\n",
    "            values.append(results[s]['class_consistency'] / 100)  # normalize to 0-1\n",
    "        values.append(1 - results[s]['training_time'] / max_time)\n",
    "        values += values[:1]\n",
    "        ax5.plot(angles, values, 'o-', linewidth=2, label=s.upper(), color=colors[s])\n",
    "        ax5.fill(angles, values, alpha=0.1, color=colors[s])\n",
    "    \n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(metrics)\n",
    "    ax5.set_title('Overall Comparison\\n(higher = better)')\n",
    "    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/benchmark_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/benchmark_comparison.png\")\n",
    "\n",
    "plot_benchmark_comparison(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747df560",
   "metadata": {},
   "source": [
    "**Figure 7.1** - Benchmark comparison across loss strategies. Top row: FID scores, KID scores with standard deviation, and training time. Bottom left: smoothed generator loss curves during training (WGAN-GP on secondary axis due to different scale). Bottom right: radar chart summarizing FID quality, KID quality, class-consistency, and training speed (higher values indicate better performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea09652",
   "metadata": {},
   "source": [
    "### 7.1.1 FID Comparison (Top-Left)\n",
    "\n",
    "The FID comparison shows a clear separation between the four loss functions.\n",
    "**WGAN-GP achieves the lowest FID (4.79)**, indicating the strongest global alignment between generated and real MNIST distributions under the adopted evaluation protocol. This result is consistent with the theoretical motivation of Wasserstein-based objectives, which aim to provide smoother gradients and a more informative distance between distributions.\n",
    "\n",
    "**BCE (5.06)** and **LSGAN (5.22)** form a strong middle tier, delivering competitive fidelity and closely approaching WGAN-GP performance.\n",
    "**Hinge loss performs worst (6.84)**, suggesting that margin-based objectives are less well adapted to the current low-resolution conditional MNIST setting and network capacity.\n",
    "\n",
    "The dashed horizontal line highlighting the best FID visually emphasizes the gap between WGAN-GP and the remaining objectives, reinforcing that, in this configuration, the theoretically grounded Wasserstein objective translates into tangible gains in sample fidelity.\n",
    "\n",
    "### 7.1.2 KID Comparison with Variance (Top-Center)\n",
    "\n",
    "The KID comparison largely mirrors the FID ranking but reveals closer performance across objectives.\n",
    "**BCE and WGAN-GP achieve the lowest KID (0.0024 and 0.0028 respectively)**, indicating strong second-order distributional alignment and faithful matching of feature statistics.\n",
    "**LSGAN (0.0028)** performs comparably to WGAN-GP, with overlapping error bars highlighting similar perceptual quality.\n",
    "\n",
    "**Hinge loss exhibits the highest KID (0.0033)** and notable uncertainty, reflecting somewhat weaker distributional alignment.\n",
    "The relatively tight clustering of KID values across all objectives suggests that, while FID reveals meaningful differences, the kernel-based metric shows more comparable performance at this scale.\n",
    "\n",
    "### 7.1.3 Training Time (Top-Right)\n",
    "\n",
    "The training time comparison reveals a pronounced computational trade-off.\n",
    "**BCE, LSGAN, and Hinge complete within a narrow time window (495â€“513 seconds)**, making them efficient and suitable for rapid prototyping and hyperparameter exploration.\n",
    "\n",
    "In contrast, **WGAN-GP requires 1857 seconds**, corresponding to roughly **3.7Ã— longer training time**. This overhead is explained by:\n",
    "- multiple critic updates per generator step (*n_critic = 5*),\n",
    "- the computational cost of gradient penalty at each discriminator update,\n",
    "- increased memory and compute overhead per iteration.\n",
    "\n",
    "In this final configuration, the increased computational cost **does translate into a clear gain in fidelity**, highlighting a classic qualityâ€“efficiency trade-off: WGAN-GP delivers the best generative performance, but at substantially higher computational expense.\n",
    "\n",
    "### 7.1.4 Generator Loss Curves (Bottom-Left)\n",
    "\n",
    "The generator loss trajectories reveal qualitatively different optimization dynamics across objectives.\n",
    "**BCE and LSGAN exhibit smooth and slowly stabilizing loss curves**, indicative of steady adversarial learning and relatively balanced generatorâ€“discriminator dynamics. BCE converges to approximately 0.8 while LSGAN stabilizes around 0.33, both showing minimal oscillation after the initial training phase.\n",
    "\n",
    "**Hinge loss shows pronounced oscillations around zero**, reflecting the sensitivity of margin-based objectives to discriminator sharpness. This noisy behavior persists throughout training, consistent with its weaker FID/KID performance.\n",
    "\n",
    "**WGAN-GP displays a strong monotonic downward trend in generator loss**, reaching approximately -34 by the end of training. While absolute loss values are not directly comparable across objectives, this smooth long-term trajectory suggests that the critic provides a consistent and informative learning signal. The stable descent correlates with the superior FID/KID obtained by WGAN-GP.\n",
    "\n",
    "### 7.1.5 Overall Comparison Radar Plot (Bottom-Right)\n",
    "\n",
    "The radar chart summarizes four dimensions: **FID quality**, **KID quality**, **class-consistency**, and **speed**.\n",
    "**WGAN-GP dominates in fidelity-related dimensions (FID and KID) and achieves the highest class-consistency (98.90%)**, but is penalized on the speed axis due to its substantially longer training time.\n",
    "\n",
    "**BCE and LSGAN show balanced profiles**, combining strong fidelity, high class-consistency (97.90% and 98.46% respectively), and computational efficiency.\n",
    "**Hinge loss remains competitive in speed but underperforms in fidelity (97.80% class-consistency)**, reinforcing the conclusion that its advantages do not materialize in this particular low-resolution conditional setting.\n",
    "\n",
    "### 7.1.6 Discussion and Implications\n",
    "\n",
    "The combined evidence from FID, KID, training time, and loss dynamics highlights a clear **qualityâ€“efficiency trade-off**.\n",
    "**WGAN-GP achieves the best overall generative quality (FID 4.79) and class-conditional fidelity (98.90%)**, validating the theoretical claims regarding Wasserstein objectives and their improved gradient properties. However, these gains come at a substantial computational cost (3.7Ã— longer training), which may be prohibitive in resource-constrained scenarios.\n",
    "\n",
    "**BCE emerges as an excellent practical choice**, achieving FID 5.06 with the fastest training time and strong class-consistency. **LSGAN performs comparably (FID 5.22)**, offering similar quality at similar cost.\n",
    "**Hinge loss appears less suitable for this specific conditional MNIST setup**, underperforming in fidelity (FID 6.84) despite similar runtime to BCE and LSGAN.\n",
    "\n",
    "These results reinforce a central message: **no GAN loss is universally optimal**. The effectiveness of each adversarial objective is highly dependent on the dataset scale, architectural capacity, and regularization strategy. Consequently, empirical benchmarking is essential, and theoretically motivated losses such as WGAN-GP should be adopted with a clear understanding of their computational and implementation trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad176712",
   "metadata": {},
   "source": [
    "## 7.2 cGAN vs DCGAN Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436415a2",
   "metadata": {},
   "source": [
    "To evaluate the impact of class conditioning on generation quality, we compare the results from conditional GAN (cGAN) and unconditional DCGAN training across all four loss strategies. Both configurations use identical architectures, hyperparameters, and training duration (30,010 steps), differing only in whether label information is provided to the Generator and Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bbe675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cGAN vs DCGAN Comparison\n",
    "\n",
    "# Results data\n",
    "strategies = ['BCE', 'LSGAN', 'Hinge', 'WGAN-GP']\n",
    "\n",
    "cgan_fid = [5.06, 5.22, 6.84, 4.79]\n",
    "dcgan_fid = [8.16, 7.17, 6.37, 6.15]\n",
    "\n",
    "cgan_kid = [0.0024, 0.0028, 0.0033, 0.0028]\n",
    "dcgan_kid = [0.0045, 0.0040, 0.0033, 0.0032]\n",
    "\n",
    "cgan_time = [495.4, 512.7, 501.7, 1856.6]\n",
    "dcgan_time = [528.6, 526.5, 523.7, 2012.2]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.35\n",
    "\n",
    "# FID Comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, cgan_fid, width, label='cGAN', color='#3498db', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, dcgan_fid, width, label='DCGAN', color='#e74c3c', alpha=0.8)\n",
    "ax1.set_ylabel('FID (lower is better)')\n",
    "ax1.set_title('FID Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(strategies)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, max(max(cgan_fid), max(dcgan_fid)) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             f'{bar.get_height():.2f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2, \n",
    "             f'{bar.get_height():.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# KID Comparison\n",
    "ax2 = axes[1]\n",
    "bars3 = ax2.bar(x - width/2, cgan_kid, width, label='cGAN', color='#3498db', alpha=0.8)\n",
    "bars4 = ax2.bar(x + width/2, dcgan_kid, width, label='DCGAN', color='#e74c3c', alpha=0.8)\n",
    "ax2.set_ylabel('KID (lower is better)')\n",
    "ax2.set_title('KID Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(strategies)\n",
    "ax2.legend()\n",
    "\n",
    "# FID Improvement (% reduction)\n",
    "ax3 = axes[2]\n",
    "fid_improvement = [(d - c) / d * 100 for c, d in zip(cgan_fid, dcgan_fid)]\n",
    "colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in fid_improvement]\n",
    "bars5 = ax3.bar(x, fid_improvement, width * 1.5, color=colors, alpha=0.8)\n",
    "ax3.set_ylabel('FID Improvement (%)')\n",
    "ax3.set_title('cGAN Improvement over DCGAN')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(strategies)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "\n",
    "for bar, val in zip(bars5, fid_improvement):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{val:+.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('images', exist_ok=True)\n",
    "plt.savefig('images/cgan_vs_dcgan_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: images/cgan_vs_dcgan_comparison.png\")\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"cGAN vs DCGAN COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Strategy':<12} {'cGAN FID':>10} {'DCGAN FID':>11} {'Improvement':>12}\")\n",
    "print(\"-\"*70)\n",
    "for i, s in enumerate(strategies):\n",
    "    imp = (dcgan_fid[i] - cgan_fid[i]) / dcgan_fid[i] * 100\n",
    "    print(f\"{s:<12} {cgan_fid[i]:>10.2f} {dcgan_fid[i]:>11.2f} {imp:>+11.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766ba9e",
   "metadata": {},
   "source": [
    "**Figure 7.2** - Comparison of cGAN and DCGAN performance across all loss strategies. Left: FID scores (lower is better). Center: KID scores (lower is better). Right: Percentage improvement of cGAN over DCGAN in FID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eb6433",
   "metadata": {},
   "source": [
    "**Key Observations**\n",
    "\n",
    "1. **Conditioning improves BCE and LSGAN significantly**: cGAN achieves 38% and 27% FID improvement respectively, demonstrating that label information provides strong guidance for these loss functions.\n",
    "\n",
    "2. **Hinge loss shows reversed pattern**: DCGAN slightly outperforms cGAN (6.37 vs 6.84 FID), suggesting that the margin-based objective may not benefit as much from explicit conditioning in this architecture.\n",
    "\n",
    "3. **WGAN-GP benefits from conditioning**: cGAN achieves the best overall FID (4.79) compared to DCGAN (6.15), a 22% improvement, confirming that the Wasserstein objective effectively leverages label information.\n",
    "\n",
    "4. **Best overall result**: cGAN with WGAN-GP achieves the lowest FID (4.79) and highest class-consistency (98.90%), validating the combination of Wasserstein training with conditional generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abc91e",
   "metadata": {},
   "source": [
    "# 8. Visualizations (Per-Class, Fixed-Z)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class quality analysis\n",
    "\n",
    "def analyze_per_class_quality(results, n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate samples for each class and compute per-class statistics.\n",
    "    Helps identify if certain digits are harder to generate.\n",
    "    \"\"\"\n",
    "    print(\"\\nPer-Class Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    class_stats = {s: {} for s in results.keys()}\n",
    "    \n",
    "    for strategy in results.keys():\n",
    "        # Load the trained generator\n",
    "        g_model = Generator().to(device)\n",
    "        g_model.load_state_dict(results[strategy]['g_model_state'])\n",
    "        g_model.eval()\n",
    "        \n",
    "        for digit in range(10):\n",
    "            noise = torch.randn(n_samples, LATENT_DIM, device=device)\n",
    "            labels = torch.full((n_samples, 1), digit, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                samples = g_model(noise, labels)\n",
    "            \n",
    "            # Compute statistics\n",
    "            mean_intensity = samples.mean().item()\n",
    "            std_intensity = samples.std().item()\n",
    "            \n",
    "            class_stats[strategy][digit] = {\n",
    "                'mean': mean_intensity,\n",
    "                'std': std_intensity,\n",
    "            }\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"\\n{'Digit':<8}\", end=\"\")\n",
    "    for s in results.keys():\n",
    "        print(f\"{s.upper():>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (8 + 12 * len(results)))\n",
    "    \n",
    "    for digit in range(10):\n",
    "        print(f\"{digit:<8}\", end=\"\")\n",
    "        for s in results.keys():\n",
    "            std = class_stats[s][digit]['std']\n",
    "            print(f\"{std:>12.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n(Values show standard deviation - higher = more variety)\")\n",
    "\n",
    "analyze_per_class_quality(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0205c2",
   "metadata": {},
   "source": [
    "## 8.1 Per-Class Diversity Analysis\n",
    "\n",
    "Table X reports the per-class standard deviation of generated samples for each digit (0â€“9) under the four adversarial objectives. Higher values indicate greater intra-class diversity, i.e., a wider range of writing styles, stroke thicknesses, and shape variations within the same digit class. This analysis complements global distributional metrics (FID/KID) by assessing how well each objective captures intra-class multimodality, which is particularly relevant in conditional generation settings.\n",
    "\n",
    "### 8.1.1 Comparison Across Loss Functions\n",
    "\n",
    "Across nearly all digits, **BCE consistently exhibits the highest intra-class diversity**, with **LSGAN** following closely, and **Hinge** and **WGAN-GP** showing comparable but slightly lower values. For example:\n",
    "\n",
    "- **Digit 0**: BCE â‰ˆ 0.695 vs. LSGAN â‰ˆ 0.686 vs. Hinge â‰ˆ 0.702 vs. WGAN-GP â‰ˆ 0.696\n",
    "- **Digit 2**: BCE â‰ˆ 0.648 vs. LSGAN â‰ˆ 0.666 vs. Hinge â‰ˆ 0.660 vs. WGAN-GP â‰ˆ 0.645\n",
    "- **Digit 8**: BCE â‰ˆ 0.660 vs. LSGAN â‰ˆ 0.659 vs. Hinge â‰ˆ 0.670 vs. WGAN-GP â‰ˆ 0.643\n",
    "\n",
    "These results indicate that all four objectives achieve relatively similar intra-class diversity in this configuration, with differences typically within 0.02â€“0.03. This convergence suggests that the architectural stabilization techniques (TTUR, Spectral Normalization for BCE/LSGAN/Hinge, LayerNorm for WGAN-GP) successfully prevent severe mode collapse across all objectives.\n",
    "\n",
    "Notably, **Hinge loss achieves the highest diversity for several digits** (0, 3, 8), despite its weaker FID performance. This indicates that the margin-based objective preserves stylistic variability but struggles with global distributional alignment.\n",
    "\n",
    "**WGAN-GP shows competitive diversity** across most digits, no longer exhibiting the severe mode contraction observed in earlier configurations. For instance:\n",
    "\n",
    "- **Digit 5**: WGAN-GP â‰ˆ 0.581 vs. BCE â‰ˆ 0.614 (moderate gap)\n",
    "- **Digit 9**: WGAN-GP â‰ˆ 0.596 vs. BCE â‰ˆ 0.612 (small gap)\n",
    "\n",
    "The improved WGAN-GP diversity reflects the benefits of the dedicated `DiscriminatorWGAN` architecture, which removes the conflicting regularization (Spectral Norm + Dropout) that previously over-constrained the critic.\n",
    "\n",
    "### 8.1.2 Class-Specific Observations\n",
    "\n",
    "Certain digits consistently admit higher diversity across all objectives:\n",
    "\n",
    "- **Digits 0, 2, 3, 6, and 8** show relatively high standard deviations across all methods (typically 0.63â€“0.70). These digits naturally allow multiple valid writing styles (e.g., open vs. closed loops for \"0\" and \"8\", varying curvature for \"2\" and \"3\"), which the models successfully capture.\n",
    "\n",
    "- **Digit 1** exhibits the lowest diversity across all methods (â‰ˆ 0.483â€“0.486), reflecting the inherently limited stylistic variability of this digit in MNIST (mostly straight vertical strokes). This low diversity appears largely independent of the chosen adversarial objective and highlights a dataset-driven limitation rather than a model-specific failure.\n",
    "\n",
    "- **Digit 7** also shows consistently lower diversity (â‰ˆ 0.573â€“0.581), likely due to its relatively constrained stroke structure.\n",
    "\n",
    "### 8.1.3 Interpretation and Link to Global Metrics\n",
    "\n",
    "The per-class diversity trends provide a nuanced interpretation of the global benchmark results:\n",
    "\n",
    "- **WGAN-GP achieves the best global fidelity (FID 4.79) and class-consistency (98.90%)** while maintaining competitive intra-class diversity. This demonstrates that, with proper architectural adaptation, Wasserstein objectives can deliver both high realism and reasonable mode coverage.\n",
    "\n",
    "- **BCE maintains strong intra-class diversity** while achieving excellent FID (5.06) and class-consistency (97.90%), indicating that classical GAN objectives, when combined with stabilization techniques, provide a robust balance between realism and diversity.\n",
    "\n",
    "- **LSGAN performs comparably to BCE** in both diversity and fidelity (FID 5.22, class-consistency 98.46%), confirming its status as a reliable and efficient objective.\n",
    "\n",
    "- **Hinge loss shows the highest diversity for some digits but the weakest global fidelity (FID 6.84)**. This suggests that the margin-based objective may preserve local stylistic variation at the expense of global distributional alignment, consistent with its noisier training dynamics observed in the loss curves.\n",
    "\n",
    "Overall, these results demonstrate that **all four objectives achieve reasonable intra-class diversity** in the final configuration, with the primary differentiator being global fidelity (FID/KID) rather than mode coverage. **WGAN-GP emerges as the best overall choice**, combining the lowest FID with competitive diversity, while **BCE offers an excellent efficiencyâ€“quality trade-off** for scenarios where training time is a constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea1767",
   "metadata": {},
   "source": [
    "## 8.2 Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated Samples Comparison Grid\n",
    "\n",
    "def plot_samples_comparison(results):\n",
    "    \"\"\"Show final generated samples from each strategy side by side.\"\"\"\n",
    "    strategies = list(results.keys())\n",
    "    n_strategies = len(strategies)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, n_strategies * 4, figsize=(n_strategies * 5, 5))\n",
    "    \n",
    "    for col, strategy in enumerate(strategies):\n",
    "        samples = results[strategy]['final_samples']\n",
    "        labels = results[strategy]['labels_test']\n",
    "        \n",
    "        for i in range(16):\n",
    "            row = i // 4\n",
    "            sub_col = i % 4\n",
    "            ax = axes[row, col * 4 + sub_col]\n",
    "            \n",
    "            img = samples[i].numpy().reshape(28, 28)\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add strategy name as title for first row\n",
    "            if i == 0:\n",
    "                ax.set_title(f'{strategy.upper()}\\n{labels[i].item()}', fontsize=10)\n",
    "            elif row == 0:\n",
    "                ax.set_title(str(labels[i].item()), fontsize=9)\n",
    "    \n",
    "    fig.suptitle('Generated Samples Comparison', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/samples_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/samples_comparison.png\")\n",
    "\n",
    "plot_samples_comparison(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c862bb",
   "metadata": {},
   "source": [
    "**Figure 8.1** - Generated samples from each loss strategy using balanced labels (digits 0-9). Each column group shows 16 samples from a single strategy, with digit labels indicated in the header row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a32950",
   "metadata": {},
   "source": [
    "### Qualitative Comparison of Generated Samples\n",
    "\n",
    "The Figure 8.1 presents representative conditional samples generated by each model after convergence. Visual inspection is consistent with the quantitative metrics reported in the benchmark.\n",
    "\n",
    "**WGAN-GP** produces the most visually consistent and structurally coherent digits across classes. The generated samples exhibit sharper contours, more stable stroke geometry, and fewer malformed shapes, particularly for curved digits such as *0*, *3*, *5*, and *8*. This qualitative improvement aligns with its superior FID and KID scores, indicating stronger global distribution matching and perceptual fidelity.\n",
    "\n",
    "**BCE** and **LSGAN** generate clear and well-formed digits with good class alignment and visible stylistic variation. While the samples are generally sharp and recognizable, subtle artifacts can be observed in some digits (e.g., slight stroke discontinuities or shape irregularities), especially for more complex classes. These minor degradations are consistent with their slightly higher FID/KID compared to WGAN-GP, suggesting competitive but marginally weaker fidelity.\n",
    "\n",
    "**Hinge loss** produces recognizable digits across all classes; however, the samples appear marginally less smooth, with occasional irregular stroke thickness and less consistent curvature. This reflects the weaker quantitative performance observed under hinge loss and supports the interpretation that margin-based objectives are less well suited to this low-resolution conditional setting.\n",
    "\n",
    "The qualitative comparison reinforces the quantitative conclusions: **WGAN-GP achieves the highest visual fidelity**, followed by **BCE and LSGAN as strong and efficient baselines**, while **Hinge loss underperforms in terms of perceptual quality**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf387195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export benchmark results to CSV\n",
    "\n",
    "def export_results_csv(results, filename='benchmark_results.csv'):\n",
    "    \"\"\"Export benchmark results to CSV for further analysis.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Strategy', 'FID', 'KID_Mean', 'KID_Std', 'Training_Time_Seconds'])\n",
    "        \n",
    "        for name, data in results.items():\n",
    "            writer.writerow([\n",
    "                name,\n",
    "                f\"{data['fid']:.4f}\",\n",
    "                f\"{data['kid_mean']:.6f}\",\n",
    "                f\"{data['kid_std']:.6f}\",\n",
    "                f\"{data['training_time']:.2f}\"\n",
    "            ])\n",
    "    \n",
    "    print(f\"Results exported to {filename}\")\n",
    "\n",
    "# export_results_csv(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22825292",
   "metadata": {},
   "source": [
    "## 8.3 Per-class Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class(g_model, rows_per_class=5, title=\"Generated Samples Per Class\"):\n",
    "    \"\"\"Generates a grid with one column per digit class (0-9).\"\"\"\n",
    "    fig, axes = plt.subplots(rows_per_class, 10, figsize=(15, 8))\n",
    "\n",
    "    for digit in range(10):\n",
    "        noise = torch.randn(rows_per_class, LATENT_DIM, device=device)\n",
    "        labels = torch.full((rows_per_class,), digit, device=device).long()  # shape: [B]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = g_model(noise, labels)\n",
    "\n",
    "        for row in range(rows_per_class):\n",
    "            img = images[row].cpu().numpy().reshape(28, 28)\n",
    "            axes[row, digit].imshow(img, cmap='gray')\n",
    "            axes[row, digit].axis('off')\n",
    "\n",
    "            if row == 0:\n",
    "                axes[row, digit].set_title(str(digit))\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot for all strategies\n",
    "for strategy_name in benchmark_results:\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(benchmark_results[strategy_name]['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    plot_per_class(g_model, title=f\"Generated Samples Per Class â€” {strategy_name.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93358",
   "metadata": {},
   "source": [
    "**Figure 8.2** - Per-class generation grid showing five samples per digit (0-9) with different noise vectors. Each column corresponds to a conditioned class, demonstrating intra-class diversity and label consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddb7f",
   "metadata": {},
   "source": [
    "## 8.4 Fixed-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a94722",
   "metadata": {},
   "source": [
    "A key property of conditional GANs is the ability to disentangle the latent noise vector z from the class label y. To verify that our cGAN has learned this separation, we fix a single noise vector and vary the conditioned label across all digits (0-9). If conditioning is correctly learned, the same z should produce digits that share stylistic characteristics (such as stroke thickness, slant, or size) while differing only in their identity. This visualization provides qualitative evidence that the generator uses z to control appearance and y to control semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fixed Z, Varying Label Visualization\n",
    "# =============================================================================\n",
    "# Demonstrates controllability: same noise vector produces different digits\n",
    "# when conditioned on different labels.\n",
    "\n",
    "def plot_fixed_z_varying_label(g_model, n_rows=4, title=\"Fixed Z, Varying Label (0-9)\"):\n",
    "    \"\"\"\n",
    "    Generate images using fixed noise vectors but varying labels.\n",
    "    Each row uses the same z, columns show digits 0-9.\n",
    "    \n",
    "    This visually demonstrates that the cGAN has learned to use the label\n",
    "    to control which digit is generated, independent of the noise vector.\n",
    "    \n",
    "    For DCGAN (unconditional), this shows different random samples instead.\n",
    "    \"\"\"\n",
    "    g_model.eval()\n",
    "    \n",
    "    if not CONDITIONAL:\n",
    "        # For unconditional (DCGAN), just show random samples\n",
    "        n_cols = 10\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 1.5))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for row in range(n_rows):\n",
    "                for col in range(n_cols):\n",
    "                    z = torch.randn(1, LATENT_DIM, device=device)\n",
    "                    sample = g_model(z, None)\n",
    "                    img = sample[0].cpu().numpy().reshape(28, 28)\n",
    "                    axes[row, col].imshow(img, cmap='gray')\n",
    "                    axes[row, col].axis('off')\n",
    "        \n",
    "        fig.suptitle(title.replace(\"Varying Label\", \"Random Samples (DCGAN)\"), fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs('images', exist_ok=True)\n",
    "        plt.savefig('images/dcgan_random_samples.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Saved: images/dcgan_random_samples.png\")\n",
    "        return\n",
    "    \n",
    "    # Conditional (cGAN) mode\n",
    "    fig, axes = plt.subplots(n_rows, 10, figsize=(15, n_rows * 1.5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row in range(n_rows):\n",
    "            # Fix one noise vector for this row\n",
    "            fixed_z = torch.randn(1, LATENT_DIM, device=device)\n",
    "            \n",
    "            for digit in range(10):\n",
    "                label = torch.tensor([digit], device=device).long()\n",
    "                \n",
    "                # Generate with fixed z, varying label\n",
    "                sample = g_model(fixed_z, label)\n",
    "                \n",
    "                img = sample[0].cpu().numpy().reshape(28, 28)\n",
    "                axes[row, digit].imshow(img, cmap='gray')\n",
    "                axes[row, digit].axis('off')\n",
    "                \n",
    "                # Column headers (digit labels)\n",
    "                if row == 0:\n",
    "                    axes[row, digit].set_title(str(digit), fontsize=12)\n",
    "            \n",
    "            # Row label\n",
    "            axes[row, 0].set_ylabel(f'z_{row+1}', fontsize=10, rotation=0, labelpad=20)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/fixed_z_varying_label.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/fixed_z_varying_label.png\")\n",
    "\n",
    "\n",
    "# Plot for all strategies\n",
    "for strategy_name in benchmark_results:\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(benchmark_results[strategy_name]['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    plot_fixed_z_varying_label(g_model, n_rows=4, title=f\"Fixed Z, Varying Label â€” {strategy_name.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb1274",
   "metadata": {},
   "source": [
    "**Figure 8.3** - Controllability demonstration using fixed noise vectors. Each row uses the same z while varying the label from 0 to 9, showing that the generator disentangles style (controlled by z) from digit identity (controlled by label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029db4d6",
   "metadata": {},
   "source": [
    "#### 8.4.1 Controllability and Conditional Disentanglement (Fixed z, Varying Label)\n",
    "\n",
    "Figure 8.3 evaluates the controllability of the conditional generator by fixing the latent vector z and varying the class label from 0 to 9. Under ideal conditional disentanglement, the digit identity should change according to the label, while the writing style (stroke thickness, curvature, slant) remains largely consistent across the row.\n",
    "\n",
    "Across all objectives, the generators successfully follow the conditioning signal, producing the correct digit identities for each label. This confirms that the label embedding and concatenation strategy is effective in enforcing class control. However, qualitative differences emerge in how well each objective preserves style consistency under fixed z:\n",
    "\n",
    "WGAN-GP exhibits the strongest style preservation across labels. For a fixed latent code, stroke thickness and overall digit morphology remain highly consistent as the label changes, indicating superior disentanglement between style (z) and content (label). This aligns with the superior quantitative performance of WGAN-GP in FID/KID and class-consistency under the updated training configuration, suggesting that the Wasserstein objective provides a more informative and smoother learning signal for conditional control.\n",
    "\n",
    "BCE and LSGAN also demonstrate clear controllability, but with slightly higher intra-row style drift. While digit identity changes as expected, subtle variations in stroke continuity and curvature are more noticeable across labels, indicating that z and label are not as cleanly disentangled as in WGAN-GP. This behavior is consistent with their competitive but inferior fidelity relative to WGAN-GP in the global metrics.\n",
    "\n",
    "Hinge loss shows the weakest controllability among the four objectives. Although class conditioning is respected, variations in stroke thickness and local digit structure are more pronounced across labels for fixed z, suggesting a noisier mapping between latent style and output appearance. This qualitative instability is coherent with the weaker FID/KID and the more oscillatory generator dynamics observed in the loss curves.\n",
    "\n",
    "This experiment highlights that WGAN-GP not only improves global distributional metrics but also strengthens conditional disentanglement, yielding more interpretable and controllable latent representations. This is particularly relevant for downstream applications (e.g., data augmentation or conditional synthesis) where fine-grained control over content versus style is desirable. The results reinforce the interpretation that, under the revised critic design and training setup, Wasserstein-based objectives can provide advantages in controllability, even when simpler losses (BCE/LSGAN) remain competitive in terms of computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817863c",
   "metadata": {},
   "source": [
    "# 9. Model Saving\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8507f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(results, save_optimizer=True):\n",
    "    \"\"\"\n",
    "    Save model checkpoints for all strategies.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary of benchmark results\n",
    "        save_optimizer: If True, saves full checkpoint with optimizer states (for resuming training).\n",
    "                       If False, saves only model weights (smaller files, for inference only).\n",
    "    \n",
    "    Checkpoint structure:\n",
    "        - model_state_dict: Generator/Discriminator weights\n",
    "        - optimizer_state_dict: Adam optimizer state (momentum buffers)\n",
    "        - step: Training step count\n",
    "        - metrics: FID, KID, class-consistency scores\n",
    "        - hyperparameters: Training configuration\n",
    "        - losses: Full loss history\n",
    "    \"\"\"\n",
    "    os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    for strategy_name, data in results.items():\n",
    "        # Save Generator checkpoint\n",
    "        g_checkpoint = {\n",
    "            \"model_state_dict\": data[\"g_model_state\"],\n",
    "            \"step\": data.get(\"step\", NUM_STEPS),\n",
    "            \"metrics\": {\n",
    "                \"fid\": data[\"fid\"],\n",
    "                \"kid_mean\": data[\"kid_mean\"],\n",
    "                \"kid_std\": data[\"kid_std\"],\n",
    "                \"class_consistency\": data[\"class_consistency\"],\n",
    "            },\n",
    "            \"hyperparameters\": data.get(\"hyperparameters\", {\"strategy\": strategy_name}),\n",
    "            \"losses\": data[\"losses\"],\n",
    "        }\n",
    "        \n",
    "        # Save Discriminator checkpoint\n",
    "        d_checkpoint = {\n",
    "            \"model_state_dict\": data[\"d_model_state\"],\n",
    "            \"step\": data.get(\"step\", NUM_STEPS),\n",
    "            \"hyperparameters\": data.get(\"hyperparameters\", {\"strategy\": strategy_name}),\n",
    "        }\n",
    "        \n",
    "        # Include optimizer states if requested (for resumable training)\n",
    "        if save_optimizer and \"g_optimizer_state\" in data:\n",
    "            g_checkpoint[\"optimizer_state_dict\"] = data[\"g_optimizer_state\"]\n",
    "            d_checkpoint[\"optimizer_state_dict\"] = data[\"d_optimizer_state\"]\n",
    "        \n",
    "        g_path = os.path.join(MODEL_OUTPUT_PATH, f\"G_{strategy_name}.pt\")\n",
    "        d_path = os.path.join(MODEL_OUTPUT_PATH, f\"D_{strategy_name}.pt\")\n",
    "        \n",
    "        torch.save(g_checkpoint, g_path)\n",
    "        torch.save(d_checkpoint, d_path)\n",
    "        \n",
    "        # Calculate file sizes\n",
    "        g_size = os.path.getsize(g_path) / (1024 * 1024)  # MB\n",
    "        d_size = os.path.getsize(d_path) / (1024 * 1024)  # MB\n",
    "        \n",
    "        opt_str = \" (with optimizer)\" if save_optimizer and \"g_optimizer_state\" in data else \" (weights only)\"\n",
    "        print(f\"Saved: {g_path} ({g_size:.1f} MB), {d_path} ({d_size:.1f} MB){opt_str}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(strategy_name, model_type=\"G\", device=None):\n",
    "    \"\"\"\n",
    "    Load a saved checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        strategy_name: Name of the strategy (e.g., \"wgan-gp\", \"bce\")\n",
    "        model_type: \"G\" for Generator, \"D\" for Discriminator\n",
    "        device: Device to load tensors to (default: current device)\n",
    "    \n",
    "    Returns:\n",
    "        checkpoint dict with keys:\n",
    "            - model_state_dict\n",
    "            - optimizer_state_dict (if saved)\n",
    "            - step\n",
    "            - metrics (Generator only)\n",
    "            - hyperparameters\n",
    "            - losses (Generator only)\n",
    "    \n",
    "    Usage:\n",
    "        # Load and restore generator\n",
    "        checkpoint = load_checkpoint(\"wgan-gp\", \"G\")\n",
    "        g_model = Generator().to(device)\n",
    "        g_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        \n",
    "        # Optionally restore optimizer for continued training\n",
    "        if \"optimizer_state_dict\" in checkpoint:\n",
    "            optimizer_g.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        \n",
    "        # Access metrics\n",
    "        print(f\"FID: {checkpoint['metrics']['fid']}\")\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    path = os.path.join(MODEL_OUTPUT_PATH, f\"{model_type}_{strategy_name}.pt\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Handle legacy format (just state_dict, not a checkpoint dict)\n",
    "    if \"model_state_dict\" not in checkpoint:\n",
    "        # Old format: file contains just the state_dict\n",
    "        checkpoint = {\"model_state_dict\": checkpoint, \"step\": 0}\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "\n",
    "save_models(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint_explanation",
   "metadata": {},
   "source": [
    "## 9.1 Checkpoint Structure\n",
    "\n",
    "Each saved checkpoint (`.pt` file) contains:\n",
    "\n",
    "| Key | Description |\n",
    "|-----|-------------|\n",
    "| `model_state_dict` | Model weights (Generator or Discriminator) |\n",
    "| `optimizer_state_dict` | Adam optimizer state including momentum buffers (optional) |\n",
    "| `step` | Training step when checkpoint was saved |\n",
    "| `metrics` | FID, KID, class-consistency scores (Generator only) |\n",
    "| `hyperparameters` | Training configuration (strategy, learning rates, etc.) |\n",
    "| `losses` | Full loss history during training (Generator only) |\n",
    "\n",
    "Including optimizer state approximately doubles file size but enables seamless training resumption with preserved momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_checkpoint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and inspect a checkpoint\n",
    "checkpoint = load_checkpoint(\"wgan-gp\", \"G\")\n",
    "\n",
    "print(f\"Training step: {checkpoint['step']}\")\n",
    "print(f\"FID: {checkpoint['metrics']['fid']:.2f}\")\n",
    "print(f\"KID: {checkpoint['metrics']['kid_mean']:.4f} Â± {checkpoint['metrics']['kid_std']:.4f}\")\n",
    "print(f\"Class-Consistency: {checkpoint['metrics']['class_consistency']:.2f}%\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for k, v in checkpoint['hyperparameters'].items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\nOptimizer state saved: {'optimizer_state_dict' in checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resume_training_md",
   "metadata": {},
   "source": [
    "## 9.2 Resuming Training from Checkpoint\n",
    "\n",
    "To continue training a model from a saved checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resume_training_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_training(strategy_name, additional_steps, save_interval=SAVE_INTERVAL):\n",
    "    \"\"\"\n",
    "    Resume training from a saved checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        strategy_name: Name of the strategy to resume (e.g., \"wgan-gp\")\n",
    "        additional_steps: Number of additional training steps\n",
    "        save_interval: How often to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Updated results dictionary\n",
    "    \"\"\"\n",
    "    # Load checkpoints\n",
    "    g_checkpoint = load_checkpoint(strategy_name, \"G\")\n",
    "    d_checkpoint = load_checkpoint(strategy_name, \"D\")\n",
    "    \n",
    "    start_step = g_checkpoint[\"step\"]\n",
    "    end_step = start_step + additional_steps\n",
    "    hp = g_checkpoint[\"hyperparameters\"]\n",
    "    \n",
    "    print(f\"Resuming {strategy_name.upper()} from step {start_step} to {end_step}\")\n",
    "    \n",
    "    # Recreate models\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(g_checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    d_model = get_discriminator(strategy_name, device)\n",
    "    d_model.load_state_dict(d_checkpoint[\"model_state_dict\"])\n",
    "    \n",
    "    # Recreate optimizers and load state\n",
    "    adam_betas = hp.get(\"adam_betas\", get_adam_betas(strategy_name))\n",
    "    optimizer_g = optim.Adam(g_model.parameters(), lr=hp.get(\"lr_g\", LR_G), betas=adam_betas)\n",
    "    optimizer_d = optim.Adam(d_model.parameters(), lr=hp.get(\"lr_d\", LR_D), betas=adam_betas)\n",
    "    \n",
    "    if \"optimizer_state_dict\" in g_checkpoint:\n",
    "        optimizer_g.load_state_dict(g_checkpoint[\"optimizer_state_dict\"])\n",
    "        optimizer_d.load_state_dict(d_checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Optimizer states restored\")\n",
    "    else:\n",
    "        print(\"Warning: No optimizer state in checkpoint, starting with fresh optimizer\")\n",
    "    \n",
    "    # Setup loss strategy\n",
    "    loss_strategy = get_loss_strategy(strategy_name, device, lambda_gp=LAMBDA_GP)\n",
    "    loss_strategy.set_d_model(d_model)\n",
    "    n_critic = loss_strategy.n_critic\n",
    "    \n",
    "    # Continue loss history\n",
    "    losses = g_checkpoint.get(\"losses\", {\"G\": [], \"D\": []})\n",
    "    \n",
    "    # Training loop\n",
    "    data_iter = iter(train_loader)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(start_step, end_step):\n",
    "        # --- Train Discriminator ---\n",
    "        for _ in range(n_critic):\n",
    "            try:\n",
    "                real_imgs, batch_labels = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                real_imgs, batch_labels = next(data_iter)\n",
    "\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_labels = batch_labels.to(device).long()\n",
    "            labels_for_g = batch_labels if CONDITIONAL else None\n",
    "\n",
    "            noise = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "            fake_imgs = g_model(noise, labels_for_g)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            d_real_out = d_model(real_imgs, labels_for_g)\n",
    "            d_loss_real = loss_strategy.d_loss_real(d_real_out)\n",
    "            d_fake_out = d_model(fake_imgs.detach(), labels_for_g)\n",
    "            d_loss_fake = loss_strategy.d_loss_fake(d_fake_out)\n",
    "            gp = loss_strategy.gradient_penalty(real_imgs, fake_imgs.detach(), labels_for_g)\n",
    "            d_loss = loss_strategy.compute_d_loss(d_loss_real, d_loss_fake, gp)\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # --- Train Generator ---\n",
    "        optimizer_g.zero_grad()\n",
    "        z = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "        gen_labels = torch.randint(0, 10, (BATCH_SIZE,), device=device).long() if CONDITIONAL else None\n",
    "        gen_imgs = g_model(z, gen_labels)\n",
    "        g_out = d_model(gen_imgs, gen_labels)\n",
    "        g_loss = loss_strategy.g_loss(g_out)\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "        losses[\"G\"].append(g_loss.item())\n",
    "        losses[\"D\"].append(d_loss.item())\n",
    "\n",
    "        if step % save_interval == 0:\n",
    "            print(f\"Step {step} â€” D: {d_loss.item():.4f}, G: {g_loss.item():.4f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nAdditional training complete in {training_time:.1f}s\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"Evaluating...\")\n",
    "    classifier_for_eval = mnist_classifier if (CLASSIFIER_AVAILABLE and CONDITIONAL) else None\n",
    "    fid_score, kid_mean, kid_std, class_acc, per_class_acc = evaluate_model_for_benchmark(g_model, classifier_for_eval)\n",
    "    \n",
    "    print(f\"\\nUpdated Results:\")\n",
    "    print(f\"  FID: {fid_score:.2f}\")\n",
    "    print(f\"  KID: {kid_mean:.4f} Â± {kid_std:.4f}\")\n",
    "    if class_acc is not None:\n",
    "        print(f\"  Class-Consistency: {class_acc:.2f}%\")\n",
    "    \n",
    "    # Return updated results\n",
    "    return {\n",
    "        \"losses\": losses,\n",
    "        \"fid\": fid_score,\n",
    "        \"kid_mean\": kid_mean,\n",
    "        \"kid_std\": kid_std,\n",
    "        \"class_consistency\": class_acc,\n",
    "        \"per_class_consistency\": per_class_acc,\n",
    "        \"training_time\": training_time,\n",
    "        \"step\": end_step,\n",
    "        \"g_model_state\": deepcopy(g_model.state_dict()),\n",
    "        \"d_model_state\": deepcopy(d_model.state_dict()),\n",
    "        \"g_optimizer_state\": deepcopy(optimizer_g.state_dict()),\n",
    "        \"d_optimizer_state\": deepcopy(optimizer_d.state_dict()),\n",
    "        \"hyperparameters\": hp,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Resume WGAN-GP training for 5000 more steps\n",
    "# resumed_results = resume_training(\"wgan-gp\", additional_steps=5000)\n",
    "# save_models({\"wgan-gp\": resumed_results})  # Save updated checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63e5cf",
   "metadata": {},
   "source": [
    "# 10. Single-Image Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a digit from 0-9\n",
    "digit = 8\n",
    "\n",
    "# Use same noise for fair comparison\n",
    "z = torch.randn(1, LATENT_DIM, device=device)\n",
    "label = torch.tensor([digit], device=device).long() \n",
    "\n",
    "fig, axes = plt.subplots(1, len(benchmark_results), figsize=(3 * len(benchmark_results), 3))\n",
    "\n",
    "for idx, (strategy_name, data) in enumerate(benchmark_results.items()):\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(data['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = g_model(z, label)\n",
    "    \n",
    "    img = generated[0].cpu().numpy().reshape(28, 28)\n",
    "    axes[idx].imshow(img, cmap='gray')\n",
    "    axes[idx].set_title(f\"{strategy_name.upper()}\\nFID: {data['fid']:.1f}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.suptitle(f\"Generated Digit: {digit}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a0983",
   "metadata": {},
   "source": [
    "**Figure 10.1** - Comparison of digit $8$ generated by each loss strategy using the same noise vector $z$. This demonstrates how different adversarial objectives produce visually distinct outputs from identical latent input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af057bb7",
   "metadata": {},
   "source": [
    "# 11. \"GAN vs. Human\" Game\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746982d1",
   "metadata": {},
   "source": [
    "![GAN game](images/gan_game.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276001db",
   "metadata": {},
   "source": [
    "**Figure 11.1** - GAN vs Human: An interactive qualitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac8e87",
   "metadata": {},
   "source": [
    "To complement the quantitative evaluation (FID, KID, class-consistency) and qualitative sample grids, we developed an interactive human-in-the-loop evaluation interface. The â€œGAN vs Humanâ€ is a game implemented as a Web application that provides an intuitive and engaging way to assess the perceptual realism and class-conditional control of the best-performing Generator (selected from the benchmark, the best WGAN-GP model) and a Classifier, trained in our previous MNIST project.\n",
    "\n",
    "During the project evaluation period, we made available the \"GAN vs. Human\" game online at https://logus2k.com/gan, allowing any users to experiment with the trained models and experience the evaluation setup firsthand.\n",
    "\n",
    "In this interface, a target digit (0â€“9) can be selected randomly by the game, or manually, by the user. Then, the Generator produces a synthetic MNIST digit conditioned on the chosen label, while user attempts to draw a more convincing MNIST digit. Both the generated sample and the user digit are subsequently evaluated by a previously trained MNIST classifier, which assigns a confidence score for the selected target class. The winner of each round is determined by which input (GAN or human) achieved the higher classifier confidence for the selected digit. Alternatively, instead of competing against the Generator, the user has the option to compete against real MNIST database digits.\n",
    "\n",
    "To further increase this challenge's interest, the game proposes three difficulty levels, by including a time pressure:\n",
    "\n",
    "- Basic: the user has 2.0 seconds to draw the selected digit;\n",
    "- Average: the user has 1.0 second to complete the drawing;\n",
    "- Pro: the user has only 0.5 seconds, emphasizing speed and motor precision.\n",
    "\n",
    "These levels progressively constrain the human input quality, allowing the comparison to probe how the GAN performs relative to increasingly noisy or incomplete human drawings.\n",
    "\n",
    "**Purpose and Evaluation Rationale**\n",
    "\n",
    "This interactive setup serves three complementary goals:\n",
    "\n",
    "1. Qualitative Realism Assessment\n",
    "It allows direct visual comparison between GAN-generated digits and human-drawn digits, making perceptual differences immediately interpretable beyond abstract metrics such as FID or KID.\n",
    "\n",
    "1. Conditional Control Verification\n",
    "By selecting the target digit manually, the experiment verifies that the conditional generator reliably responds to class inputs in real time, reinforcing the controllability analysis presented earlier (fixed z, varying label).\n",
    "\n",
    "1. Human vs. Model Benchmarking\n",
    "The competitive framing highlights how well the generatorâ€™s outputs align with the learned decision boundaries of the classifier. In several trials, the GAN achieves classifier confidence comparable to (and sometimes exceeding) that of human drawings, illustrating that the generator has internalized salient class-discriminative features of MNIST.\n",
    "\n",
    "The â€œGAN vs Humanâ€ game provides an intuitive and communicative demonstration of the generatorâ€™s capabilities, complementing the formal benchmarking results and offering an accessible way to showcase conditional generation performance to both technical and non-technical audiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4a005",
   "metadata": {},
   "source": [
    "# 12. Limitations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa3d45a",
   "metadata": {},
   "source": [
    "Despite the strong empirical results achieved in this project, particularly with the final WGAN-GP configuration (FID = **4.79**, KID = **0.0028 Â± 0.0015**, Class-Consistency = **98.90%**), several limitations remain, both methodological and conceptual. These limitations also motivate a number of promising directions for future work that could extend the project from a strong experimental implementation to a more advanced and research-oriented generative modeling framework.\n",
    "\n",
    "### 12.1 Dataset Simplicity and Scalability\n",
    "\n",
    "All experiments were conducted on MNIST, a low-resolution (28Ã—28), grayscale dataset with relatively low visual complexity and limited semantic variability. While MNIST remains useful for controlled experimentation and methodological comparisons, it does not reflect the challenges encountered in higher-dimensional image generation tasks (e.g., CIFAR-10, CelebA, FFHQ). Consequently, the conclusions drawn about the relative performance of adversarial objectives may not fully generalize to more complex datasets, where architectural choices, conditioning strategies, and regularization methods interact differently.\n",
    "\n",
    "### 12.2 Metric Bias and Domain Mismatch\n",
    "\n",
    "FID and KID were computed using Inception features, despite Inception being trained on natural images (ImageNet) and MNIST lying far outside that domain. Although these metrics were used consistently across all models and thus remain valid for **relative comparison**, their absolute values should not be interpreted as universal indicators of perceptual realism. Similarly, the \"GAN vs Human\" interactive evaluation relies on a classifier trained on MNIST, which introduces a closed-loop bias: the generator is effectively evaluated against a model whose decision boundaries resemble those encountered during training. This may overestimate perceptual realism while underrepresenting human subjective judgment.\n",
    "\n",
    "### 12.3 Objective Sensitivity and Regularization Interactions\n",
    "\n",
    "The benchmark revealed that WGAN-GP is highly sensitive to architectural and regularization choices. Although the final configuration achieved the best quantitative results, earlier variants exhibited degraded performance due to unfavorable interactions between gradient penalty, Spectral Normalization, and Dropout. This necessitated the development of a dedicated `DiscriminatorWGAN` architecture that removes these conflicting mechanisms in favor of LayerNorm and increased capacity. This highlights that theoretically motivated objectives do not automatically translate into superior empirical performance without careful adaptation of the discriminator architecture and training dynamics.\n",
    "\n",
    "### 12.4 Training Reproducibility and CUDA Non-Determinism\n",
    "\n",
    "During development, significant challenges arose from CUDA non-determinism, which caused identical code with the same random seed to produce vastly different training trajectories across runs. This was resolved by enforcing deterministic behavior (`torch.backends.cudnn.deterministic = True`, `torch.backends.cudnn.benchmark = False`), but at the cost of reduced training speed. This experience underscores the importance of controlling for hardware-level randomness when reporting GAN benchmarks and highlights a often-overlooked reproducibility concern in deep learning research.\n",
    "\n",
    "### 12.5 Limited Latent Space Interpretability\n",
    "\n",
    "While class-controllability was verified (fixed *z*, varying label), the structure of the latent space itself was not systematically analyzed. As a result, little is known about how semantic attributes (e.g., stroke thickness, curvature, slant) are organized internally, or whether latent directions correspond to interpretable factors of variation. This limits interpretability and deeper understanding of what the generator has learned beyond class identity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f2e38",
   "metadata": {},
   "source": [
    "# 13. Future Work\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebaffdf",
   "metadata": {},
   "source": [
    "Building on the strong performance of the final WGAN-GP model, several extensions could further strengthen both the scientific depth and practical relevance of the project.\n",
    "\n",
    "### 13.1 Latent Space Analysis\n",
    "\n",
    "Now that the generator has learned a high-quality data manifold, analyzing the geometry of its latent space becomes both feasible and informative:\n",
    "\n",
    "- **Latent Interpolation**: Interpolating between two latent vectors for the same class can reveal whether stylistic attributes (e.g., slanted vs. upright digits, thick vs. thin strokes) change smoothly, providing insight into the continuity of the learned manifold.\n",
    "- **Label Interpolation**: Although labels are discrete, interpolating between class embeddings (e.g., between â€˜3â€™ and â€˜8â€™) can expose how the generator behaves in ambiguous semantic regions and whether it produces meaningful hybrid shapes or collapses to dominant modes.\n",
    "\n",
    "### 13.2 Architectural Refinements\n",
    "\n",
    "More advanced conditioning mechanisms could be explored:\n",
    "\n",
    "- **Projection Discriminator**: Replacing label-channel concatenation with a projection discriminator could improve class-conditional modeling by directly coupling class embeddings with discriminator features. This approach is known to scale better to higher-resolution and multi-class datasets.\n",
    "- **Self-Attention Modules**: Introducing self-attention layers in the generator and discriminator (e.g., at 14Ã—14 or 28Ã—28 resolutions) may improve global coherence by enabling long-range spatial dependencies, which become increasingly important in more complex datasets.\n",
    "\n",
    "### 13.3 Advanced Regularization and Training Strategies\n",
    "\n",
    "Further stabilization techniques could be incorporated:\n",
    "\n",
    "- **Exponential Moving Average (EMA) of Generator Weights**: Maintaining an EMA copy of the generator often leads to significantly improved FID and smoother visual quality at evaluation time.\n",
    "- **Differentiable Data Augmentation (DiffAugment)**: Applying lightweight augmentations to both real and generated samples before discrimination could improve robustness and reduce overfitting, particularly when scaling to smaller datasets or higher resolutions.\n",
    "\n",
    "### 13.4 Robustness and Diversity Evaluation\n",
    "\n",
    "Beyond global fidelity metrics, deeper evaluation of diversity and failure modes would strengthen the analysis:\n",
    "\n",
    "- **MS-SSIM for Intra-class Diversity**: Measuring diversity within each digit class can help detect partial mode collapse, ensuring that high class-consistency does not come at the expense of stylistic variability.\n",
    "- **Boundary and Failure-Mode Analysis**: Using the classifier to probe borderline or ambiguous samples could reveal systematic weaknesses in the generator, offering insight into how decision boundaries shape generative outputs.\n",
    "\n",
    "### 13.5 Deployment and Optimization\n",
    "\n",
    "Finally, practical deployment aspects could be explored:\n",
    "\n",
    "- **Model Quantization**: Evaluating the impact of 8-bit quantization on FID, inference speed, and visual quality would provide insight into deployment feasibility on resource-constrained devices.\n",
    "- **ONNX Export and Cross-Platform Inference**: Exporting the generator to ONNX would enable broader usability in production or web-based applications, such as extending the â€œGAN vs Humanâ€ game to real-time interactive demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dee79",
   "metadata": {},
   "source": [
    "# 14. Conclusions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7cecc",
   "metadata": {},
   "source": [
    "This project presented a comprehensive empirical study of conditional Generative Adversarial Networks (cGANs) for digit generation on the MNIST dataset, with a particular focus on how different adversarial loss functions influence training stability, sample fidelity, class controllability, and computational efficiency. By benchmarking BCE, LSGAN, Hinge, and WGAN-GP under a controlled experimental setup, the work provides a clear and systematic comparison of classical and modern GAN objectives within the same architectural and training framework.\n",
    "\n",
    "The results show that WGAN-GP, when carefully adapted with a dedicated critic architecture and appropriate regularization, achieved the strongest overall performance in terms of distributional fidelity and controllability, reaching the lowest FID (4.79) and KID (0.0028 Â± 0.0015), together with the highest class-consistency (98.90%). This confirms the theoretical promise of Wasserstein-based objectives when the Lipschitz constraint is enforced correctly and when architectural choices are aligned with the training objective. However, the benchmarking process also highlighted that WGAN-GP is highly sensitive to design choices and computationally expensive, requiring approximately 3.7Ã— longer training time than the other objectives. This reinforces an important practical insight: advanced GAN objectives do not offer \"plug-and-play\" improvements and demand careful architectural and hyperparameter adaptation to outperform simpler alternatives.\n",
    "\n",
    "In contrast, BCE and LSGAN emerged as strong and reliable baselines, achieving competitive fidelity, high class-consistency, and substantially lower computational cost. BCE achieved FID 5.06 with the fastest training time (495s), while LSGAN reached FID 5.22 with slightly higher class-consistency (98.46%). Both objectives consistently offered an excellent trade-off between stability, realism, and efficiency, making them highly practical choices for conditional generation in low-resolution domains. These findings underline that classical GAN losses remain highly competitive when combined with modern stabilization techniques such as TTUR, Spectral Normalization, and conditional inputs. This challenges the common assumption that more sophisticated losses are universally superior and highlights the importance of empirical validation over theoretical preference alone.\n",
    "\n",
    "The qualitative analyses further strengthened these conclusions. Per-class diversity measurements showed that all four objectives achieved comparable intra-class variability in the final configuration, with BCE and Hinge showing slightly higher diversity for certain digits. The controllability experiments demonstrated that the models successfully disentangled style (latent code) from class identity, validating the conditional design. The interactive GAN vs Human evaluation provided an intuitive demonstration of the generator's realism, showing that the best-performing model can produce digits competitive with human-drawn samples under a trained classifier. At the same time, this experiment also exposed the limitations of classifier-based evaluation, reminding that high classifier confidence does not necessarily imply perceptual realism from a human perspective.\n",
    "\n",
    "The comparison between cGAN and DCGAN configurations revealed that conditioning significantly improves performance for BCE (+38% FID improvement) and LSGAN (+27%), while WGAN-GP also benefits substantially (+22%). Interestingly, Hinge loss showed a slight degradation with conditioning (-7%), suggesting that margin-based objectives may not leverage label information as effectively in this architectural setting.\n",
    "\n",
    "Overall, this work highlights three key lessons. First, the effectiveness of a GAN objective is inseparable from the surrounding training pipelineâ€”architecture, normalization, regularization, and optimization strategy jointly determine performance. Second, empirical benchmarking is essential, as theoretical advantages do not automatically translate into superior results in practical settings. Third, even on a relatively simple dataset such as MNIST, conditional GANs exhibit rich trade-offs between fidelity, diversity, stability, and efficiency, making careful design and evaluation crucial.\n",
    "\n",
    "These findings provide a solid foundation for extending the framework to more complex datasets and architectures, and for exploring more advanced conditioning and regularization strategies in future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95d5a5",
   "metadata": {},
   "source": [
    "# 15. References\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c46e6",
   "metadata": {},
   "source": [
    "1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. https://www.deeplearningbook.org/\n",
    "2. PyTorch. *DCGAN Tutorial*. https://docs.pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
