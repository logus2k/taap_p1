{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888e9661",
   "metadata": {},
   "source": [
    "# Draft_01_v8\n",
    "\n",
    "**Course:** Advanced Topics in Deep Learning  \n",
    "**Topic:** Generative Adversarial Networks (GANs)  \n",
    "**Authors:** Ant√≥nio Cruz (140129), C√°tia Br√°s (120093), Ricardo Kayseller (95813)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe4d2f",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Generative Adversarial Networks (GANs) have become a fundamental class of generative models for learning complex data distributions and synthesizing realistic samples. In the classical (unconditional) GAN framework, the generator receives only random noise as input and learns to produce samples that resemble the training data, while the discriminator learns to distinguish real data from generated samples. Although this setting allows the model to generate realistic images, it provides no control over the semantic content of the generated samples. In the context of datasets with labeled structure, such as MNIST, an unconditional GAN may generate any digit between 0 and 9, but the user cannot specify which digit should be produced.\n",
    "\n",
    "Conditional Generative Adversarial Networks (cGANs) extend the original GAN formulation by incorporating additional side information, such as class labels, into both the generator and the discriminator. By conditioning the generation process on a label ùë¶, the generator learns a mapping ùê∫(ùëß,ùë¶) that aims to produce samples consistent with the desired class, while the discriminator is trained to assess not only whether an image is real or fake, but also whether it matches the provided condition. This conditioning mechanism enables controlled generation and significantly increases the practical usefulness of GANs in applications where semantic attributes matter, such as digit synthesis, object class control, and, more generally, text-to-image generation.\n",
    "\n",
    "In this project, we investigate the implementation and behavior of conditional GANs on the MNIST dataset of handwritten digits. Starting from an unconditional DCGAN baseline, we progressively introduce conditioning mechanisms that allow explicit control over the generated digit class. Beyond basic conditioning via concatenation of labels, we explore stabilization strategies inspired by the literature, such as Spectral Normalization applied to the discriminator, in order to mitigate training instabilities and mode collapse. The experimental analysis focuses on both qualitative and quantitative aspects, including visual inspection of generated samples, class-conditional control checks, intra-class diversity, and discriminator behavior during training.\n",
    "\n",
    "The main objective of this work is to demonstrate that conditional adversarial training enables controllable image generation, to analyze the limitations of simple conditioning strategies, and to assess how architectural and training refinements improve stability, diversity, and label consistency. This study provides practical insights into the challenges of training cGANs and highlights the importance of appropriate conditioning and regularization techniques for achieving reliable class-conditional generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf50d1",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60594529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required dependencies\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.kid import KernelInceptionDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7147d6",
   "metadata": {},
   "source": [
    "## 1.2 Global Configuration and Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "LIVE_MONITOR = False\n",
    "EMIT_INTERVAL = 1\n",
    "\n",
    "DATASET_PATH = \"../../dataset/\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 100\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Path to calibrated classifier checkpoint\n",
    "CLASSIFIER_CHECKPOINT = \"../../drafts/draft_01/classifier/mnist_cnn_calibrated_best.ckpt\"\n",
    "\n",
    "# NUM_STEPS = 15005 corresponds to ~32 epochs (60000 images / 128 batch_size ‚âà 469 steps/epoch)\n",
    "NUM_STEPS = 15005\n",
    "SAVE_INTERVAL = 1000\n",
    "\n",
    "# TTUR: Two Time-Scale Update Rule\n",
    "# D learns faster than G, so we use different learning rates\n",
    "LR_D = 4e-4  # Discriminator learning rate\n",
    "LR_G = 1e-4  # Generator learning rate (4x slower)\n",
    "\n",
    "# Adam betas optimized for GAN training\n",
    "# Lower Œ≤1 (0.5 vs default 0.9) reduces momentum, stabilizes adversarial updates\n",
    "ADAM_BETAS = (0.5, 0.999)\n",
    "\n",
    "# Label smoothing for BCE/LSGAN (use 0.9 instead of 1.0 for real labels)\n",
    "LABEL_SMOOTHING_REAL = 0.9\n",
    "\n",
    "# WGAN-GP: number of critic steps per generator step\n",
    "N_CRITIC = 5\n",
    "\n",
    "MODEL_OUTPUT_PATH = \"model/\"\n",
    "D_MODEL_NAME = \"D_DRAFT_01\"\n",
    "G_MODEL_NAME = \"G_DRAFT_01\"\n",
    "\n",
    "NUM_EVAL_SAMPLES = 10000\n",
    "\n",
    "# Strategies to benchmark\n",
    "BENCHMARK_STRATEGIES = [\"bce\", \"lsgan\", \"hinge\", \"wgan-gp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e67525",
   "metadata": {},
   "source": [
    "### 1.2.1 Setup and Training Strategy\n",
    "\n",
    "This project adopts a systematic experimental design to analyze the behavior and stability of conditional Generative Adversarial Networks (cGANs) on the MNIST dataset. Beyond implementing a single adversarial formulation, multiple loss strategies are benchmarked, including the standard binary cross-entropy (BCE) loss, Least Squares GAN (LSGAN), hinge loss, and Wasserstein GAN with Gradient Penalty (WGAN-GP). This design choice follows the theoretical and empirical insights discussed in the lecture materials on GAN losses and training stability, which highlight that different adversarial objectives lead to substantially different optimization dynamics, convergence behavior, and robustness to mode collapse. In particular, WGAN-GP is included due to its smoother loss landscape and explicit enforcement of the Lipschitz constraint, which is known to improve training stability compared to classical GAN formulations.\n",
    "\n",
    "Training is performed using the Two Time-Scale Update Rule (TTUR), where the discriminator is updated with a learning rate four times higher than the generator (LR_D = 4√ó10‚Åª‚Å¥, LR_G = 1√ó10‚Åª‚Å¥). This choice is directly motivated by best practices discussed in the GAN training ‚Äúrecipe‚Äù guidelines, which emphasize that an overly weak discriminator fails to provide informative gradients to the generator, while an overly strong discriminator can lead to vanishing gradients. The Adam optimizer is used with Œ≤‚ÇÅ = 0.5 and Œ≤‚ÇÇ = 0.999, following the DCGAN and subsequent GAN literature, as these hyperparameters are empirically known to stabilize adversarial training by reducing excessive momentum in the discriminator updates.\n",
    "\n",
    "The batch size is set to 128, representing a compromise between training stability and computational efficiency. Larger batch sizes generally provide more stable gradient estimates for both generator and discriminator, while still fitting comfortably within GPU memory constraints. The dimensionality of the latent space is fixed to 100, following common practice in DCGAN-style architectures, which provides sufficient capacity for modeling the variability of handwritten digits without introducing unnecessary complexity.\n",
    "\n",
    "Training is defined in terms of a fixed number of optimization steps rather than epochs. Specifically, the model is trained for 15,005 steps. This value originates from the reference Keras implementation that was converted to PyTorch and corresponds approximately to 32 epochs over the MNIST training set (MNIST contains 60,000 samples, and with a batch size of 128, one epoch corresponds to roughly 469 iterations). The step-based formulation is consistent with how GAN experiments are typically reported in the literature, as it provides finer control over the balance between generator and discriminator updates, enables predictable checkpointing and sampling intervals, and facilitates fair comparisons across different training configurations. The additional offset of five steps ensures that the final checkpoint aligns exactly with the predefined sampling interval of 1,000 steps, allowing the evolution of generated samples to be consistently monitored at fixed milestones throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe64063",
   "metadata": {},
   "source": [
    "### 1.2.2 Live Training Monitor for GAN Benchmarking (Real-Time Dashboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LIVE_MONITOR:\n",
    "    from bin.gan_monitor import (\n",
    "        start_server, emit_frames, emit_done,\n",
    "        emit_benchmark_start, emit_strategy_start, emit_strategy_end\n",
    "    )\n",
    "    start_server(port=8992)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59932440",
   "metadata": {},
   "source": [
    "A real-time monitoring dashboard was integrated into the training pipeline to support continuous inspection of adversarial dynamics during benchmarking. When enabled, a lightweight local server streams training diagnostics such as generator sample grids, discriminator and generator loss trajectories, and per-strategy progress markers. This design aligns with best-practice recommendations for GAN training, where losses alone are often insufficient to assess convergence or detect failure modes. The live monitor improves experimental transparency, facilitates early detection of mode collapse or imbalance between networks, and ensures consistent reporting across multiple loss strategies within the same benchmarking framework.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility across both numpy and pytorch\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b60dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available, otherwise fall back to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bacc40",
   "metadata": {},
   "source": [
    "### 2. Modular GAN Loss Strategies (BCE, LSGAN, Hinge, WGAN-GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Strategies\n",
    "\n",
    "class GANLossStrategy(ABC):\n",
    "    \"\"\"Base class for GAN loss strategies.\"\"\"\n",
    "    use_sigmoid: bool = True\n",
    "    use_label_smoothing: bool = False  # only BCE/LSGAN use this\n",
    "    n_critic: int = 1  # D steps per G step\n",
    "    smooth_real: float = 1.0\n",
    "    \n",
    "    @abstractmethod\n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Discriminator loss for real images.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Discriminator loss for fake images.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        \"\"\"Generator loss.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def gradient_penalty(self, real_imgs: Tensor, fake_imgs: Tensor, labels: Tensor) -> Tensor:\n",
    "        \"\"\"Gradient penalty (only used by WGAN-GP).\"\"\"\n",
    "        return torch.tensor(0.0, device=real_imgs.device)\n",
    "    \n",
    "    def set_d_model(self, d_model) -> None:\n",
    "        \"\"\"Set discriminator reference (used by WGAN-GP).\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_d_loss(self, d_loss_real: Tensor, d_loss_fake: Tensor, gp: Tensor) -> Tensor:\n",
    "        \"\"\"Combine D losses. Override for strategy-specific formulas.\"\"\"\n",
    "        return 0.5 * (d_loss_real + d_loss_fake) + gp\n",
    "\n",
    "\n",
    "class BCELossStrategy(GANLossStrategy):\n",
    "    \"\"\"Binary Cross-Entropy loss (original GAN) with label smoothing.\"\"\"\n",
    "    use_sigmoid = True\n",
    "    use_label_smoothing = True\n",
    "    \n",
    "    def __init__(self, device, smooth_real: float = 0.9):\n",
    "        self.device = device\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.smooth_real = smooth_real\n",
    "        self._real_labels = None\n",
    "        self._fake_labels = None\n",
    "    \n",
    "    def _ensure_labels(self, batch_size):\n",
    "        if self._real_labels is None or self._real_labels.size(0) != batch_size:\n",
    "            self._real_labels = torch.full((batch_size, 1), self.smooth_real, device=self.device)\n",
    "            self._fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._real_labels)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._fake_labels)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        # G wants D to output 1.0 (no smoothing for G)\n",
    "        ones = torch.ones(output.size(0), 1, device=self.device)\n",
    "        return self.criterion(output, ones)\n",
    "\n",
    "\n",
    "class LSGANLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Least Squares loss with label smoothing.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    use_label_smoothing = True\n",
    "    \n",
    "    def __init__(self, device, smooth_real: float = 0.9):\n",
    "        self.device = device\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.smooth_real = smooth_real\n",
    "        self._real_labels = None\n",
    "        self._fake_labels = None\n",
    "    \n",
    "    def _ensure_labels(self, batch_size):\n",
    "        if self._real_labels is None or self._real_labels.size(0) != batch_size:\n",
    "            self._real_labels = torch.full((batch_size, 1), self.smooth_real, device=self.device)\n",
    "            self._fake_labels = torch.zeros(batch_size, 1, device=self.device)\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._real_labels)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        self._ensure_labels(output.size(0))\n",
    "        return self.criterion(output, self._fake_labels)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        ones = torch.ones(output.size(0), 1, device=self.device)\n",
    "        return self.criterion(output, ones)\n",
    "\n",
    "\n",
    "class HingeLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Hinge loss ‚Äî used in SAGAN, BigGAN.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(F.relu(1.0 - output))\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(F.relu(1.0 + output))\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "\n",
    "\n",
    "class WGANGPLossStrategy(GANLossStrategy):\n",
    "    \"\"\"Wasserstein loss with gradient penalty.\"\"\"\n",
    "    use_sigmoid = False\n",
    "    n_critic = 5  # train D 5 times per G step\n",
    "    \n",
    "    def __init__(self, lambda_gp: float = 10.0):\n",
    "        self.d_model = None\n",
    "        self.lambda_gp = lambda_gp\n",
    "    \n",
    "    def set_d_model(self, d_model) -> None:\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def d_loss_real(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "    \n",
    "    def d_loss_fake(self, output: Tensor) -> Tensor:\n",
    "        return torch.mean(output)\n",
    "    \n",
    "    def g_loss(self, output: Tensor) -> Tensor:\n",
    "        return -torch.mean(output)\n",
    "    \n",
    "    def gradient_penalty(self, real_imgs: Tensor, fake_imgs: Tensor, labels: Tensor) -> Tensor:\n",
    "        if self.d_model is None:\n",
    "            raise RuntimeError(\"d_model not set. Call set_d_model() first.\")\n",
    "        \n",
    "        batch_size = real_imgs.size(0)\n",
    "        \n",
    "        alpha = torch.rand(batch_size, 1, 1, 1, device=real_imgs.device)\n",
    "        interpolated = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
    "        \n",
    "        d_out = self.d_model(interpolated, labels)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_out,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_out),\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        \n",
    "        return self.lambda_gp * ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    def compute_d_loss(self, d_loss_real: Tensor, d_loss_fake: Tensor, gp: Tensor) -> Tensor:\n",
    "        \"\"\"WGAN-GP uses d_fake - d_real + gp (no 0.5 averaging).\"\"\"\n",
    "        return d_loss_fake + d_loss_real + gp  # note: d_loss_real is already negated\n",
    "\n",
    "\n",
    "def get_loss_strategy(name: str, device) -> GANLossStrategy:\n",
    "    \"\"\"Factory function to get loss strategy by name.\"\"\"\n",
    "    strategies = {\n",
    "        \"bce\": BCELossStrategy,\n",
    "        \"lsgan\": LSGANLossStrategy,\n",
    "        \"hinge\": HingeLossStrategy,\n",
    "        \"wgan-gp\": WGANGPLossStrategy,\n",
    "    }\n",
    "    if name not in strategies:\n",
    "        raise ValueError(f\"Unknown loss strategy: {name}. Options: {list(strategies.keys())}\")\n",
    "    \n",
    "    if name == \"bce\":\n",
    "        return strategies[name](device, smooth_real=LABEL_SMOOTHING_REAL)\n",
    "    if name == \"lsgan\":\n",
    "        return strategies[name](device, smooth_real=LABEL_SMOOTHING_REAL)\n",
    "    return strategies[name]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3805d",
   "metadata": {},
   "source": [
    "#### 2.1 GAN Loss Strategies and Benchmarking Framework\n",
    "\n",
    "In order to systematically study the impact of different adversarial objectives on training stability and sample quality, the training pipeline was designed around a modular loss-strategy framework. Instead of hard-coding a single GAN loss, a common abstract interface was defined to encapsulate the discriminator and generator loss components, enabling multiple loss formulations to be benchmarked under identical architectural and optimization conditions.\n",
    "\n",
    "Four widely adopted loss strategies were implemented and evaluated:\n",
    "\n",
    "1. **Binary Cross-Entropy (BCE) Loss**  \n",
    "   This strategy corresponds to the original GAN formulation, where the discriminator is trained as a probabilistic classifier distinguishing real from generated samples. The generator is optimized to maximize the probability that fake samples are classified as real. To improve training stability, label smoothing is applied to real samples, reducing overconfidence in the discriminator and mitigating sharp gradients that may destabilize the adversarial dynamics. This formulation directly reflects the classical minimax game described in the original GAN literature.\n",
    "\n",
    "2. **Least Squares GAN (LSGAN)**  \n",
    "   The LSGAN objective replaces the binary cross-entropy loss with a least-squares regression loss. Instead of learning to output hard binary decisions, the discriminator is encouraged to regress towards continuous target values for real and fake samples. This formulation has been shown to reduce vanishing gradients and produce smoother optimization landscapes, often resulting in more stable convergence during training. As in the BCE strategy, label smoothing is applied to the real targets to further regularize the discriminator.\n",
    "\n",
    "3. **Hinge Loss**  \n",
    "   The hinge loss formulation adopts a margin-based objective, where the discriminator enforces a separation between real and fake samples using a hinge function. Rather than predicting explicit probabilities, the discriminator outputs real-valued scores, and the generator is trained to maximize these scores for generated samples. This loss is commonly used in modern high-performance GAN architectures and aligns with the design patterns discussed in contemporary GAN literature, particularly in combination with architectural constraints such as spectral normalization.\n",
    "\n",
    "4. **Wasserstein GAN with Gradient Penalty (WGAN-GP)**  \n",
    "   The WGAN-GP strategy reformulates the adversarial game in terms of the Wasserstein distance between the real and generated data distributions. Instead of a classifier, the discriminator acts as a critic that assigns scalar scores to samples. To enforce the required Lipschitz continuity constraint, a gradient penalty term is added, penalizing deviations of the gradient norm from unity along linear interpolations between real and fake samples. Additionally, the critic is updated multiple times per generator update, following the two time-scale update principle, to ensure a sufficiently strong approximation of the Wasserstein distance during training.\n",
    "\n",
    "By implementing these loss strategies within a unified interface, the project enables a fair and controlled comparison of fundamentally different adversarial objectives. All strategies share the same generator and discriminator architectures, optimization settings, and training schedule, ensuring that observed differences in convergence behavior, stability, and sample quality can be attributed primarily to the choice of loss function rather than to confounding implementation details. This benchmarking setup reflects the theoretical and practical considerations emphasized in the course materials, where the choice of adversarial loss is a central factor in the stability and effectiveness of GAN training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25edd84b",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: convert PIL image to tensor (scales [0,255] to [0,1]),\n",
    "# then normalize to [-1, 1] range using mean=0.5, std=0.5\n",
    "# Formula: (x - 0.5) / 0.5 = 2x - 1, which maps [0,1] to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load MNIST dataset from a local folder\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=DATASET_PATH,\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# DataLoader handles batching, shuffling, and parallel loading\n",
    "# drop_last=True discards the final incomplete batch so every batch has exactly BATCH_SIZE samples\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac413f3",
   "metadata": {},
   "source": [
    "### 3.1 Data Loading and Preprocessing (MNIST ‚Üí Tensor ‚Üí Normalized to [-1, 1])\n",
    "\n",
    "MNIST images were converted to tensors and normalized to the range [-1, 1] using a mean of 0.5 and standard deviation of 0.5. This preprocessing step is required when the generator outputs images through a Tanh activation, ensuring that real and generated samples lie on the same numerical scale. The training split contains 60,000 images, and batching was performed with shuffling enabled. The DataLoader used drop_last=True to maintain a constant batch size across iterations, simplifying adversarial training dynamics and logging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716cfa9",
   "metadata": {},
   "source": [
    "## 4. Generator and Discriminator\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590216d7",
   "metadata": {},
   "source": [
    "### 4.1 Conditional Generator (PixelShuffle Upsampling + ICNR Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.label_embedding = nn.Embedding(num_classes, latent_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 7 * 7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Block 1: 7x7 -> 14x14\n",
    "        self.conv1 = nn.Conv2d(128, 128 * 4, kernel_size=3, padding=1)\n",
    "        self.ps1 = nn.PixelShuffle(2)\n",
    "        self.bn1 = nn.BatchNorm2d(128, momentum=0.8)\n",
    "\n",
    "        # Block 2: 14x14 -> 28x28\n",
    "        self.conv2 = nn.Conv2d(128, 64 * 4, kernel_size=3, padding=1)\n",
    "        self.ps2 = nn.PixelShuffle(2)\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=0.8)\n",
    "\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        self._icnr_init(self.conv1, upscale_factor=2)\n",
    "        self._icnr_init(self.conv2, upscale_factor=2)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _icnr_init(self, conv_layer, upscale_factor):\n",
    "        \"\"\"Specialized initialization to prevent checkerboard/dots.\"\"\"\n",
    "        new_shape = [\n",
    "            conv_layer.out_channels // (upscale_factor**2),\n",
    "            conv_layer.in_channels,\n",
    "            conv_layer.kernel_size[0],\n",
    "            conv_layer.kernel_size[1]\n",
    "        ]\n",
    "        sub_kernel = torch.randn(new_shape) * 0.02\n",
    "        # Repeat the sub-kernel across the 'sub-pixel' channels\n",
    "        # This makes all 4 pixels in a 2x2 block start identical\n",
    "        icnr_kernel = sub_kernel.repeat_interleave(upscale_factor**2, dim=0)\n",
    "        conv_layer.weight.data.copy_(icnr_kernel)\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        label_embed = self.label_embedding(labels)  # labels shape: [B]\n",
    "        x = self.fc(z * label_embed)\n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "\n",
    "        x = F.relu(self.bn1(self.ps1(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.ps2(self.conv2(x))))\n",
    "        return torch.tanh(self.output_conv(x))\n",
    "\n",
    "\n",
    "# Instantiate and move to device\n",
    "g_model = Generator().to(device)\n",
    "print(g_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072ba5e",
   "metadata": {},
   "source": [
    "The conditional generator was implemented using a PixelShuffle-based upsampling architecture. Class conditioning was introduced through a learned embedding of the digit label into the latent space, combined multiplicatively with the noise vector to modulate the generation process. The model first projects the conditioned latent code into a 7√ó7√ó128 feature map, then performs two stages of sub-pixel upsampling (7‚Üí14‚Üí28) using convolution + PixelShuffle blocks with Batch Normalization and ReLU activations. To reduce checkerboard and dot artifacts that can emerge from upsampling, ICNR initialization was applied to the convolutional layers preceding PixelShuffle. Finally, the generator outputs a 28√ó28 grayscale image through a Tanh activation, which is consistent with preprocessing that normalizes MNIST images to the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856fd90",
   "metadata": {},
   "source": [
    "### 4.2 Conditional Discriminator (Label Map Concatenation + Spectral Normalization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=NUM_CLASSES, use_sigmoid=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_sigmoid = use_sigmoid\n",
    "\n",
    "        # Embed the class label into a vector of size 28*28\n",
    "        self.label_embedding = nn.Embedding(num_classes, 28 * 28)\n",
    "\n",
    "        # Main sequential network with Spectral Normalization\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is (2, 28, 28): image channel + label channel\n",
    "            spectral_norm(nn.Conv2d(2, 32, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Second conv block: (32, 14, 14) ‚Üí (64, 7, 7)\n",
    "            spectral_norm(nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Flatten\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Dense layers with Spectral Normalization\n",
    "            spectral_norm(nn.Linear(64 * 7 * 7, 512)),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            spectral_norm(nn.Linear(512, 1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        batch_size = img.size(0)\n",
    "        \n",
    "        # Embed label and reshape to spatial map\n",
    "        label_embed = self.label_embedding(labels)  # labels shape: [B] -> [B, 784]\n",
    "        label_embed = label_embed.view(batch_size, 1, 28, 28)\n",
    "\n",
    "        # Concatenate image and label map\n",
    "        x = torch.cat([img, label_embed], dim=1)\n",
    "\n",
    "        x = self.model(x)\n",
    "        \n",
    "        if self.use_sigmoid:\n",
    "            x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d1bce",
   "metadata": {},
   "source": [
    "The conditional discriminator was implemented using a label-map concatenation strategy, where each class label was embedded into a 28√ó28 spatial map and concatenated with the input image as an additional channel. The architecture consisted of two spectral-normalized convolutional blocks that progressively downsampled the input (28‚Üí14‚Üí7), followed by a spectral-normalized multilayer perceptron that produced a single authenticity score. Spectral normalization was applied to stabilize adversarial training by constraining the discriminator‚Äôs sensitivity, reducing oscillations and improving robustness to mode collapse. For benchmarking across different adversarial objectives (BCE, LSGAN, hinge, and WGAN-GP), the discriminator was configured to output raw scores (logits), allowing each loss strategy to apply its appropriate formulation without relying on a fixed sigmoid output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3db53b",
   "metadata": {},
   "source": [
    "### 4.3 Weight Initialization (DCGAN-Style + Safe Handling for Spectral Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d951fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    DCGAN-style weight initialization.\n",
    "    Skip spectral_norm wrapped layers (they handle their own init).\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1 and not hasattr(m, 'weight_orig'):\n",
    "        # Conv layer without spectral norm\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1 and not hasattr(m, 'weight_orig'):\n",
    "        # Linear layer without spectral norm\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc3f8d",
   "metadata": {},
   "source": [
    "Model parameters were initialized following the DCGAN guidelines, which recommend sampling convolutional and fully connected weights from a Normal(0, 0.02) distribution and initializing batch normalization layers with weights drawn from Normal(1, 0.02) and zero bias. This initialization scheme has been shown to promote stable adversarial dynamics during the early stages of training and to reduce the likelihood of vanishing gradients or premature mode collapse.\n",
    "\n",
    "Label embedding layers were initialized using the same Normal(0, 0.02) distribution to ensure comparable scale between latent and conditional representations. For layers wrapped with Spectral Normalization, initialization was applied to the underlying unnormalized weight parameters (weight_orig) rather than the normalized weights, ensuring consistent parameter scaling at initialization while preserving the Lipschitz constraint enforced during training. This design choice aligns with best practices in stabilizing GAN training, as discussed in the DCGAN and SNGAN literature, where careful weight initialization and spectral normalization jointly contribute to improved convergence behavior and reduced training oscillations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cec7284",
   "metadata": {},
   "source": [
    "# 6. Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e30af",
   "metadata": {},
   "source": [
    "\n",
    "### 6.1 Class Consistency Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures % of generated images where classifier prediction matches conditioned label.\n",
    "# Uses the same calibrated classifier from the game for consistency.\n",
    "\n",
    "# Classifier architecture (must match checkpoint)\n",
    "class MNISTCNNCalibrated(nn.Module):\n",
    "    \"\"\"Calibrated CNN for MNIST - architecture only, no Lightning dependencies.\"\"\"\n",
    "    \n",
    "    def __init__(self, width=128, depth=3, dropout_p=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: depth blocks of Conv(3x3) -> ReLU -> MaxPool(2)\n",
    "        channels = [width, 2 * width, 2 * width][:depth]\n",
    "        in_ch = 1\n",
    "        blocks = []\n",
    "        for out_ch in channels:\n",
    "            blocks.append(nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=True))\n",
    "            blocks.append(nn.ReLU())\n",
    "            blocks.append(nn.MaxPool2d(kernel_size=2))\n",
    "            in_ch = out_ch\n",
    "        self.encoder = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Head: flatten -> FC(width) -> ReLU -> dropout -> FC(num_classes)\n",
    "        spatial = 28 // (2 ** depth)\n",
    "        feat_dim = channels[-1] * spatial * spatial\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feat_dim, width),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(width, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.head(self.encoder(x))\n",
    "\n",
    "\n",
    "def load_calibrated_classifier(checkpoint_path):\n",
    "    \"\"\"Load the calibrated classifier from checkpoint.\"\"\"\n",
    "    classifier = MNISTCNNCalibrated(width=128, depth=3, dropout_p=0.1)\n",
    "    \n",
    "    # Load checkpoint (Lightning format)\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    state_dict = ckpt['state_dict']\n",
    "    \n",
    "    # Remove 'model.' prefix if present (Lightning adds this sometimes)\n",
    "    clean_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        clean_key = k.replace('model.', '') if k.startswith('model.') else k\n",
    "        clean_state[clean_key] = v\n",
    "    \n",
    "    classifier.load_state_dict(clean_state)\n",
    "    classifier.to(device)\n",
    "    classifier.eval()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def evaluate_class_consistency(g_model, classifier, samples_per_class=500):\n",
    "    \"\"\"\n",
    "    Evaluate class-consistency accuracy.\n",
    "    \n",
    "    Returns:\n",
    "        overall_accuracy: % of generated images where prediction matches conditioned label\n",
    "        per_class_accuracy: List of accuracies for each digit (0-9)\n",
    "    \"\"\"\n",
    "    classifier.eval()\n",
    "    g_model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    per_class_correct = [0] * 10\n",
    "    per_class_total = [0] * 10\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            labels = torch.full((samples_per_class,), digit, device=device).long()\n",
    "            noise = torch.randn(samples_per_class, LATENT_DIM, device=device)\n",
    "            \n",
    "            # Generate images (output range: [-1, 1])\n",
    "            fake_imgs = g_model(noise, labels)\n",
    "            \n",
    "            # Convert to classifier input range [0, 1]\n",
    "            fake_imgs_normalized = (fake_imgs + 1) / 2\n",
    "            \n",
    "            # Classify\n",
    "            logits = classifier(fake_imgs_normalized)\n",
    "            predicted = logits.argmax(dim=1)\n",
    "            \n",
    "            # Count matches\n",
    "            matches = (predicted == labels).sum().item()\n",
    "            correct += matches\n",
    "            total += samples_per_class\n",
    "            per_class_correct[digit] = matches\n",
    "            per_class_total[digit] = samples_per_class\n",
    "    \n",
    "    overall_accuracy = 100.0 * correct / total\n",
    "    per_class_accuracy = [100.0 * per_class_correct[i] / per_class_total[i] for i in range(10)]\n",
    "    \n",
    "    return overall_accuracy, per_class_accuracy\n",
    "\n",
    "\n",
    "# Load classifier\n",
    "print(\"Loading calibrated classifier for class-consistency evaluation...\")\n",
    "try:\n",
    "    mnist_classifier = load_calibrated_classifier(CLASSIFIER_CHECKPOINT)\n",
    "    print(f\"Classifier loaded from: {CLASSIFIER_CHECKPOINT}\")\n",
    "    \n",
    "    # Verify classifier accuracy on real MNIST\n",
    "    classifier_correct = 0\n",
    "    classifier_total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in train_loader:\n",
    "            # Note: train_loader images are [-1, 1], convert to [0, 1]\n",
    "            imgs = ((imgs + 1) / 2).to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = mnist_classifier(imgs)\n",
    "            predicted = logits.argmax(dim=1)\n",
    "            classifier_total += labels.size(0)\n",
    "            classifier_correct += (predicted == labels).sum().item()\n",
    "    print(f\"Classifier accuracy on real MNIST: {100.0 * classifier_correct / classifier_total:.2f}%\")\n",
    "    CLASSIFIER_AVAILABLE = True\n",
    "except FileNotFoundError:\n",
    "    print(f\"WARNING: Classifier checkpoint not found at {CLASSIFIER_CHECKPOINT}\")\n",
    "    print(\"Class-consistency evaluation will be skipped.\")\n",
    "    mnist_classifier = None\n",
    "    CLASSIFIER_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ed24c",
   "metadata": {},
   "source": [
    "**Model Instantiation, Loss Strategy Selection**\n",
    "\n",
    "Models were instantiated on the selected compute device, with the generator using its internal initialization (including ICNR initialization for PixelShuffle upsampling) and the discriminator initialized using DCGAN-style weight initialization to improve early training stability. A modular loss-strategy interface was used to select the adversarial objective (BCE, LSGAN, hinge, or WGAN-GP). For WGAN-GP, the discriminator instance was passed into the strategy object to enable computation of the gradient penalty term, enforcing the Lipschitz constraint required by the Wasserstein formulation. Training used the Two Time-Scale Update Rule (TTUR), configuring separate Adam optimizers for discriminator and generator with a higher discriminator learning rate (LR_D > LR_G) and GAN-optimized momentum parameters (Œ≤‚ÇÅ=0.5, Œ≤‚ÇÇ=0.999). This setup supports stable adversarial dynamics and enables fair benchmarking across different loss formulations under consistent optimization conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e8547",
   "metadata": {},
   "source": [
    "**Class-Consistency Evaluation (Label-Alignment Metric)**\n",
    "\n",
    "To complement distribution-based metrics (FID/KID) and qualitative inspection, we introduced a class-consistency evaluation to explicitly measure whether the conditional generator obeys the requested label. This metric quantifies label alignment by computing the percentage of generated images whose predicted class (from an external classifier) matches the conditioning label ùë¶. This is particularly relevant for conditional GANs on MNIST because a model can produce visually plausible digits while still ignoring conditioning information (a known failure mode in cGANs).\n",
    "\n",
    "Metric definition:\n",
    "- For each class ùë¶‚àà{0,‚Ä¶,9}, we generate ùëÅ samples using ùê∫(ùëß,ùë¶), then evaluate them with a pretrained MNIST classifier ùê∂(‚ãÖ). \n",
    "\n",
    "We also report per-class consistency, which is useful to detect asymmetric failures (e.g., digits that collapse into visually similar classes such as 3/5/8 or 4/9).\n",
    "\n",
    "**Why this evaluation is important**\n",
    "\n",
    "- *Directly targets the conditioning objective: Unlike FID/KID, this metric evaluates whether the generator respects the provided label.*\n",
    "\n",
    "- *Detects ‚Äúconditional collapse‚Äù: The generator may learn a narrow subset of digits and still appear stable, but class-consistency immediately exposes label leakage or label ignoring.*\n",
    "\n",
    "- *MNIST-appropriate: A digit classifier is trained on the same domain (handwritten digits), making it a strong and interpretable control metric.*\n",
    "\n",
    "Classifier choice and calibration:\n",
    "We used the same calibrated MNIST classifier employed previously (in the project pipeline) to ensure consistency across experiments. The classifier architecture (CNN) was re-instantiated without Lightning dependencies, and the weights were loaded from a Lightning-style checkpoint (state_dict). A minor compatibility step was included to remove a potential 'model.' prefix in checkpoint keys, which commonly appears depending on how the model was saved.\n",
    "\n",
    "Before using the classifier for evaluation, we verified that it achieved high accuracy on real MNIST to ensure it is a reliable oracle for class-consistency. If the checkpoint is missing, the evaluation is safely skipped (to avoid breaking the benchmark pipeline).\n",
    "\n",
    "This evaluation was applied to each trained generator configuration (different loss strategies / regularization setups), enabling a clear comparison of how each strategy affects label compliance, not only visual fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd3a42",
   "metadata": {},
   "source": [
    "### 6.2 Benchmark Loop: Training Multiple GAN Losses with Real-Time Monitoring and Unified Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a066dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark training loop\n",
    "\n",
    "# Store results\n",
    "benchmark_results = {}\n",
    "\n",
    "def run_benchmark(strategies=BENCHMARK_STRATEGIES, num_steps=NUM_STEPS, save_interval=SAVE_INTERVAL):\n",
    "    \"\"\"\n",
    "    Run training for each loss strategy and collect metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Emit benchmark start\n",
    "    if LIVE_MONITOR:\n",
    "        emit_benchmark_start(strategies, num_steps)\n",
    "    \n",
    "    for strategy_idx, strategy_name in enumerate(strategies):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING: {strategy_name.upper()}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Emit strategy start\n",
    "        if LIVE_MONITOR:\n",
    "            emit_strategy_start(strategy_name, strategy_idx, len(strategies))\n",
    "        \n",
    "        # Reset random seeds for fair comparison\n",
    "        np.random.seed(SEED)\n",
    "        torch.manual_seed(SEED)\n",
    "        \n",
    "        # Instantiate fresh models - get strategy first to use its use_sigmoid\n",
    "        g_model = Generator().to(device)\n",
    "        loss_strategy = get_loss_strategy(strategy_name, device)\n",
    "        d_model = Discriminator(use_sigmoid=loss_strategy.use_sigmoid).to(device)\n",
    "        d_model.apply(weights_init)\n",
    "        loss_strategy.set_d_model(d_model)\n",
    "        \n",
    "        # Optimizers\n",
    "        optimizer_d = optim.Adam(d_model.parameters(), lr=LR_D, betas=ADAM_BETAS)\n",
    "        optimizer_g = optim.Adam(g_model.parameters(), lr=LR_G, betas=ADAM_BETAS)\n",
    "        \n",
    "        # Training state\n",
    "        losses = {\"G\": [], \"D\": []}\n",
    "        data_iter = iter(train_loader)\n",
    "        n_critic = loss_strategy.n_critic\n",
    "        d_loss = torch.tensor(0.0)\n",
    "        \n",
    "        # Fixed test samples for visualization\n",
    "        samples_test = torch.randn(16, LATENT_DIM, device=device)\n",
    "        # Balanced labels for monitoring (0-9, 0-5) ensures all digits represented\n",
    "        labels_test = torch.arange(0, 10, device=device).repeat(2)[:16].long()  # shape: [B]\n",
    "        \n",
    "        # Timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            # --- Train Discriminator ---\n",
    "            for _ in range(n_critic):\n",
    "                try:\n",
    "                    real_imgs, batch_labels = next(data_iter)\n",
    "                except StopIteration:\n",
    "                    data_iter = iter(train_loader)\n",
    "                    real_imgs, batch_labels = next(data_iter)\n",
    "\n",
    "                real_imgs = real_imgs.to(device)\n",
    "                batch_labels = batch_labels.to(device).long()  # shape: [B]\n",
    "\n",
    "                noise = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "                fake_imgs = g_model(noise, batch_labels)\n",
    "\n",
    "                optimizer_d.zero_grad()\n",
    "                d_real_out = d_model(real_imgs, batch_labels)\n",
    "                d_loss_real = loss_strategy.d_loss_real(d_real_out)\n",
    "                d_fake_out = d_model(fake_imgs.detach(), batch_labels)\n",
    "                d_loss_fake = loss_strategy.d_loss_fake(d_fake_out)\n",
    "                gp = loss_strategy.gradient_penalty(real_imgs, fake_imgs.detach(), batch_labels)\n",
    "                d_loss = loss_strategy.compute_d_loss(d_loss_real, d_loss_fake, gp)\n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            # --- Train Generator ---\n",
    "            optimizer_g.zero_grad()\n",
    "            z = torch.randn(BATCH_SIZE, LATENT_DIM, device=device)\n",
    "            gen_labels = torch.randint(0, 10, (BATCH_SIZE,), device=device).long()  # shape: [B]\n",
    "            gen_imgs = g_model(z, gen_labels)\n",
    "            g_out = d_model(gen_imgs, gen_labels)\n",
    "            g_loss = loss_strategy.g_loss(g_out)\n",
    "            g_loss.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            losses[\"G\"].append(g_loss.item())\n",
    "            losses[\"D\"].append(d_loss.item())\n",
    "\n",
    "            if step % save_interval == 0:\n",
    "                print(f\"Step {step} ‚Äî D: {d_loss.item():.4f}, G: {g_loss.item():.4f}\")\n",
    "\n",
    "            # Live monitor\n",
    "            if LIVE_MONITOR and step % EMIT_INTERVAL == 0:\n",
    "                with torch.no_grad():\n",
    "                    monitor_samples = g_model(samples_test, labels_test)\n",
    "                emit_frames(monitor_samples, labels_test, step, g_loss.item(), d_loss.item(), num_steps)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"\\nEvaluating {strategy_name}...\")\n",
    "        fid_score, kid_mean, kid_std, class_acc, per_class_acc = evaluate_model_for_benchmark(g_model, mnist_classifier if CLASSIFIER_AVAILABLE else None)\n",
    "        \n",
    "        # Emit strategy end with results\n",
    "        if LIVE_MONITOR:\n",
    "            emit_strategy_end(strategy_name, fid_score, kid_mean, kid_std, training_time)\n",
    "        \n",
    "        # Generate final samples\n",
    "        with torch.no_grad():\n",
    "            final_samples = g_model(samples_test, labels_test)\n",
    "        \n",
    "        # Store results\n",
    "        results[strategy_name] = {\n",
    "            \"losses\": losses,\n",
    "            \"fid\": fid_score,\n",
    "            \"kid_mean\": kid_mean,\n",
    "            \"kid_std\": kid_std,\n",
    "            \"class_consistency\": class_acc,\n",
    "            \"per_class_consistency\": per_class_acc,\n",
    "            \"training_time\": training_time,\n",
    "            \"g_model_state\": deepcopy(g_model.state_dict()),\n",
    "            \"d_model_state\": deepcopy(d_model.state_dict()),\n",
    "            \"final_samples\": final_samples.cpu(),\n",
    "            \"labels_test\": labels_test.cpu(),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{strategy_name.upper()} Results:\")\n",
    "        print(f\"  FID: {fid_score:.2f}\")\n",
    "        print(f\"  KID: {kid_mean:.4f} ¬± {kid_std:.4f}\")\n",
    "        if class_acc is not None:\n",
    "            print(f\"  Class-Consistency: {class_acc:.2f}%\")\n",
    "        print(f\"  Time: {training_time:.1f}s\")\n",
    "    \n",
    "    # Emit benchmark complete\n",
    "    if LIVE_MONITOR:\n",
    "        emit_done()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model_for_benchmark(g_model, classifier=None):\n",
    "    \"\"\"Evaluation function for benchmarking (returns values, doesn't print).\"\"\"\n",
    "    fid = FrechetInceptionDistance(feature=2048).to(device)\n",
    "    kid = KernelInceptionDistance(feature=2048, subset_size=100).to(device)\n",
    "\n",
    "    idx = np.random.randint(0, len(train_dataset), NUM_EVAL_SAMPLES)\n",
    "    batch_size = 256\n",
    "    \n",
    "    for i in range(0, NUM_EVAL_SAMPLES, batch_size):\n",
    "        batch_idx = idx[i:i + batch_size]\n",
    "        real_batch = torch.stack([train_dataset[j][0] for j in batch_idx])\n",
    "        real_batch = ((real_batch + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "        real_batch = real_batch.repeat(1, 3, 1, 1)\n",
    "        real_batch = torch.nn.functional.interpolate(\n",
    "            real_batch.float(), size=(299, 299), mode='bilinear', align_corners=False\n",
    "        ).to(torch.uint8).to(device)\n",
    "        fid.update(real_batch, real=True)\n",
    "        kid.update(real_batch, real=True)\n",
    "\n",
    "    for i in range(0, NUM_EVAL_SAMPLES, batch_size):\n",
    "        current_batch = min(batch_size, NUM_EVAL_SAMPLES - i)\n",
    "        noise = torch.randn(current_batch, LATENT_DIM, device=device)\n",
    "        labels = torch.randint(0, 10, (current_batch,), device=device).long()  # shape: [B]\n",
    "        with torch.no_grad():\n",
    "            fake_batch = g_model(noise, labels)\n",
    "        fake_batch = ((fake_batch + 1) / 2 * 255).clamp(0, 255).to(torch.uint8)\n",
    "        fake_batch = fake_batch.repeat(1, 3, 1, 1)\n",
    "        fake_batch = torch.nn.functional.interpolate(\n",
    "            fake_batch.float(), size=(299, 299), mode='bilinear', align_corners=False\n",
    "        ).to(torch.uint8).to(device)\n",
    "        fid.update(fake_batch, real=False)\n",
    "        kid.update(fake_batch, real=False)\n",
    "\n",
    "    fid_score = fid.compute().item()\n",
    "    kid_mean, kid_std = kid.compute()\n",
    "    \n",
    "    # Class-consistency evaluation\n",
    "    if classifier is not None:\n",
    "        class_acc, per_class_acc = evaluate_class_consistency(g_model, classifier)\n",
    "    else:\n",
    "        class_acc, per_class_acc = None, None\n",
    "    \n",
    "    return fid_score, kid_mean.item(), kid_std.item(), class_acc, per_class_acc\n",
    "\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_results = run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923fdc9",
   "metadata": {},
   "source": [
    "Comparative Analysis of Loss Functions\n",
    "Experimental Setup\n",
    "\n",
    "All adversarial objectives (BCE, LSGAN, Hinge, and WGAN-GP) were evaluated under a controlled and identical architectural setup. In particular, the Discriminator shared the same network design across all experiments, incorporating:\n",
    "\n",
    "- Spectral Normalization (SN) applied to all convolutional and fully connected layers;\n",
    "\n",
    "- Dropout (p = 0.25) as a regularization mechanism;\n",
    "\n",
    "- Two Time-Scale Update Rule (TTUR), with the Discriminator learning rate four times larger than that of the Generator;\n",
    "\n",
    "- Conditional input via spatial concatenation of the class embedding with the image.\n",
    "\n",
    "This ensured a fair comparison across loss strategies, while also imposing a strong regularization regime on the Discriminator.\n",
    "\n",
    "##### Sample Fidelity and Distributional Alignment (FID & KID)\n",
    "\n",
    "The quantitative evaluation using FID and KID revealed a clear ranking among the adversarial objectives:\n",
    "\n",
    "- LSGAN achieved the best overall performance, with FID = 6.30 and KID = 0.0034 ¬± 0.0017, indicating the strongest alignment between generated and real MNIST distributions.\n",
    "\n",
    "- BCE followed closely, with FID = 7.74 and KID = 0.0048 ¬± 0.0020, confirming its role as a strong baseline for low-resolution image generation.\n",
    "\n",
    "- Hinge loss exhibited a noticeable degradation, obtaining FID = 10.44 and KID = 0.0070 ¬± 0.0022, suggesting that margin-based objectives may require higher model capacity or different regularization to be competitive in this setting.\n",
    "\n",
    "- WGAN-GP performed substantially worse, with FID = 68.03 and KID = 0.0691 ¬± 0.0078, indicating poor distributional matching under the adopted configuration.\n",
    "\n",
    "These results suggest that, for MNIST and the current cGAN architecture, least-squares objectives provide more stable and effective gradients for training than Wasserstein-based formulations.\n",
    "\n",
    "##### Conditional Consistency\n",
    "\n",
    "Beyond visual fidelity, the conditional nature of the model was evaluated via a class-consistency metric, measuring the percentage of generated samples whose predicted class matched the conditioning label:\n",
    "\n",
    "- BCE: 97.64%\n",
    "\n",
    "- LSGAN: 97.38%\n",
    "\n",
    "- Hinge: 97.18%\n",
    "\n",
    "- WGAN-GP: 71.02%\n",
    "\n",
    "The high consistency scores obtained by BCE, LSGAN, and Hinge confirm that the conditioning mechanism is correctly learned and that the Generator reliably respects the class information. In contrast, the markedly lower consistency of WGAN-GP indicates that, in this regime, the Wasserstein objective fails to preserve class-conditional structure, leading to semantically inconsistent samples.\n",
    "\n",
    "##### Computational Cost\n",
    "\n",
    "The computational overhead varied significantly across strategies:\n",
    "\n",
    "- BCE / LSGAN / Hinge: approximately 280‚Äì290 seconds of training time.\n",
    "\n",
    "- WGAN-GP: approximately 1,360 seconds, reflecting the additional cost of multiple critic updates per generator step (n_critic = 5) and the computation of the gradient penalty.\n",
    "\n",
    "Thus, WGAN-GP incurred a substantially higher computational cost without corresponding gains in sample quality or conditional consistency in this experimental setting.\n",
    "\n",
    "##### On the Degradation of WGAN-GP under SN + Dropout\n",
    "\n",
    "The poor performance of WGAN-GP should be interpreted in light of the interaction between multiple regularization mechanisms applied simultaneously to the Discriminator:\n",
    "\n",
    "- Spectral Normalization (SN) explicitly constrains the Lipschitz constant of the Discriminator.\n",
    "\n",
    "- Gradient Penalty (GP) enforces an additional Lipschitz constraint by penalizing deviations of the gradient norm from unity.\n",
    "\n",
    "- Dropout introduces stochasticity into the Discriminator‚Äôs function, perturbing the smooth gradient structure assumed by the Wasserstein formulation.\n",
    "\n",
    "The combination of SN + GP + Dropout can therefore lead to over-regularization of the Discriminator, limiting its capacity to provide informative gradients to the Generator. Since WGAN-GP relies critically on well-behaved and accurate gradient estimates, this mismatch degrades both visual fidelity and conditional alignment. In contrast, BCE and LSGAN objectives are less sensitive to such constraints and remain stable under strong regularization, which explains their superior empirical performance in this setup.\n",
    "\n",
    "The benchmark indicates that for conditional MNIST generation under a strongly regularized Discriminator, LSGAN offers the most favorable trade-off between fidelity, conditional consistency, and computational efficiency, with BCE remaining a competitive baseline. Although WGAN-GP is theoretically attractive for high-dimensional and complex image distributions, its direct application in this low-resolution, heavily regularized setting proved suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc049e",
   "metadata": {},
   "source": [
    "### 6.3 Benchmark Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary table\n",
    "\n",
    "def print_results_table(results):\n",
    "    \"\"\"Print a formatted comparison table.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Strategy':<12} {'FID':>10} {'KID':>18} {'Time (s)':>12}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Sort by FID (best first)\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['fid'])\n",
    "    \n",
    "    for i, (name, data) in enumerate(sorted_results):\n",
    "        rank = \"ü•á\" if i == 0 else \"ü•à\" if i == 1 else \"ü•â\" if i == 2 else \"  \"\n",
    "        kid_str = f\"{data['kid_mean']:.4f} ¬± {data['kid_std']:.4f}\"\n",
    "        print(f\"{rank} {name:<10} {data['fid']:>10.2f} {kid_str:>18} {data['training_time']:>12.1f}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Best performer\n",
    "    best = sorted_results[0]\n",
    "    print(f\"\\n‚úì Best performer: {best[0].upper()} (FID: {best[1]['fid']:.2f})\")\n",
    "\n",
    "print_results_table(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b53b28",
   "metadata": {},
   "source": [
    "Interpretation of Results\n",
    "\n",
    "LSGAN achieved the best overall performance, obtaining the lowest FID (6.30) and KID (0.0034), indicating the closest alignment between generated and real MNIST distributions. Compared to BCE, which achieved FID = 7.74 and KID = 0.0048, the improvement is consistent with the theoretical motivation of least-squares objectives to mitigate gradient saturation and provide smoother optimization dynamics for the Generator.\n",
    "\n",
    "BCE remained a strong baseline, producing competitive fidelity scores with only a small degradation relative to LSGAN. This confirms that the original GAN objective remains effective for low-resolution domains such as MNIST when combined with architectural stabilization techniques (Spectral Normalization, Dropout, TTUR).\n",
    "\n",
    "Hinge loss showed a clear drop in performance, with FID = 10.44 and KID = 0.0070. While hinge-based objectives are widely used in high-capacity GANs (e.g., BigGAN, SAGAN), their benefits did not translate as effectively to this lower-resolution setting and architecture, suggesting that hinge loss may require either larger model capacity or alternative regularization schemes to be competitive.\n",
    "\n",
    "WGAN-GP performed significantly —Ö—É–∂–µ (worse), with FID = 68.03 and KID = 0.0691, while also being by far the most computationally expensive method (‚âà 1,360 seconds vs. ‚âà 280‚Äì290 seconds for the others). This indicates that, under the current configuration, the Wasserstein objective with gradient penalty fails to provide useful learning signals for conditional MNIST generation.\n",
    "\n",
    "##### Computational Trade-offs\n",
    "\n",
    "The training time highlights an important practical trade-off:\n",
    "\n",
    "- BCE, LSGAN, and Hinge converge in comparable time (‚âà 280‚Äì290 s),\n",
    "\n",
    "- WGAN-GP is almost 5√ó slower, due to:\n",
    "\n",
    "1. multiple critic updates per generator step (n_critic = 5),\n",
    "\n",
    "2. the cost of computing gradient penalties,\n",
    "\n",
    "3. higher memory and computational overhead.\n",
    "\n",
    "Given that WGAN-GP also produced substantially worse sample quality, it is dominated by the simpler objectives in this experimental regime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5470cae",
   "metadata": {},
   "source": [
    "### 7. Evaluation of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization\n",
    "\n",
    "def plot_benchmark_comparison(results):\n",
    "    \"\"\"Create comprehensive comparison visualizations.\"\"\"\n",
    "    strategies = list(results.keys())\n",
    "    n_strategies = len(strategies)\n",
    "    \n",
    "    # Color scheme\n",
    "    colors = {'bce': '#3498db', 'lsgan': '#2ecc71', 'hinge': '#e74c3c', 'wgan-gp': '#9b59b6'}\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # --- 1. FID Bar Chart (top left) ---\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    x = np.arange(n_strategies)\n",
    "    width = 0.35\n",
    "    \n",
    "    fids = [results[s]['fid'] for s in strategies]\n",
    "    \n",
    "    bars1 = ax1.bar(x, fids, width*1.5, color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax1.set_ylabel('FID (lower is better)')\n",
    "    ax1.set_title('FID Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([s.upper() for s in strategies])\n",
    "    ax1.axhline(y=min(fids), color='green', linestyle='--', alpha=0.5, label=f'Best: {min(fids):.2f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    for bar, val in zip(bars1, fids):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # --- 2. KID Bar Chart (top middle) ---\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    kid_means = [results[s]['kid_mean'] for s in strategies]\n",
    "    kid_stds = [results[s]['kid_std'] for s in strategies]\n",
    "    \n",
    "    bars2 = ax2.bar(x, kid_means, width*1.5, yerr=kid_stds, capsize=5,\n",
    "                    color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax2.set_ylabel('KID (lower is better)')\n",
    "    ax2.set_title('KID Comparison (with std)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([s.upper() for s in strategies])\n",
    "    \n",
    "    # --- 3. Training Time (top right) ---\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    times = [results[s]['training_time'] for s in strategies]\n",
    "    bars3 = ax3.bar(x, times, width*1.5, color=[colors[s] for s in strategies], alpha=0.8)\n",
    "    ax3.set_ylabel('Time (seconds)')\n",
    "    ax3.set_title('Training Time')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels([s.upper() for s in strategies])\n",
    "    \n",
    "    for bar, val in zip(bars3, times):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{val:.0f}s', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # --- 4. Loss Curves with Dual Y-Axis (bottom left, spans 2 columns) ---\n",
    "    ax4 = fig.add_subplot(2, 3, (4, 5))\n",
    "    \n",
    "    # Smoothing function\n",
    "    def smooth(data, window=100):\n",
    "        if len(data) > window:\n",
    "            return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        return data\n",
    "    \n",
    "    # Plot BCE, LSGAN, Hinge on left axis\n",
    "    for s in ['bce', 'lsgan', 'hinge']:\n",
    "        if s in results:\n",
    "            losses_g = results[s]['losses']['G']\n",
    "            smoothed = smooth(losses_g)\n",
    "            ax4.plot(smoothed, label=f'{s.upper()} (G)', color=colors[s], linewidth=1.5)\n",
    "    \n",
    "    ax4.set_xlabel('Step')\n",
    "    ax4.set_ylabel('Generator Loss (BCE / LSGAN / Hinge)')\n",
    "    ax4.set_ylim(-0.5, 1.5)\n",
    "    ax4.legend(loc='upper left')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_title('Generator Loss Curves')\n",
    "    \n",
    "    # Plot WGAN-GP on right axis\n",
    "    if 'wgan-gp' in results:\n",
    "        ax4_right = ax4.twinx()\n",
    "        losses_g = results['wgan-gp']['losses']['G']\n",
    "        smoothed = smooth(losses_g)\n",
    "        ax4_right.plot(smoothed, label='WGAN-GP (G)', color=colors['wgan-gp'], \n",
    "                       linewidth=1.5, linestyle='--')\n",
    "        ax4_right.set_ylabel('Generator Loss (WGAN-GP)', color=colors['wgan-gp'])\n",
    "        ax4_right.tick_params(axis='y', labelcolor=colors['wgan-gp'])\n",
    "        ax4_right.legend(loc='upper right')\n",
    "    \n",
    "    # --- 5. Radar/Spider Chart (bottom right) ---\n",
    "    ax5 = fig.add_subplot(2, 3, 6, projection='polar')\n",
    "    \n",
    "    max_fid = max(fids)\n",
    "    max_kid = max(kid_means)\n",
    "    max_time = max(times)\n",
    "    \n",
    "    # Check if class-consistency is available\n",
    "    has_class_consistency = all(results[s].get('class_consistency') is not None for s in strategies)\n",
    "    \n",
    "    if has_class_consistency:\n",
    "        metrics = ['FID\\nQuality', 'KID\\nQuality', 'Class\\nConsistency', 'Speed']\n",
    "    else:\n",
    "        metrics = ['FID\\nQuality', 'KID\\nQuality', 'Speed']\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    for s in strategies:\n",
    "        values = [\n",
    "            1 - results[s]['fid'] / max_fid,\n",
    "            1 - results[s]['kid_mean'] / max_kid,\n",
    "        ]\n",
    "        if has_class_consistency:\n",
    "            values.append(results[s]['class_consistency'] / 100)  # normalize to 0-1\n",
    "        values.append(1 - results[s]['training_time'] / max_time)\n",
    "        values += values[:1]\n",
    "        ax5.plot(angles, values, 'o-', linewidth=2, label=s.upper(), color=colors[s])\n",
    "        ax5.fill(angles, values, alpha=0.1, color=colors[s])\n",
    "    \n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(metrics)\n",
    "    ax5.set_title('Overall Comparison\\n(higher = better)')\n",
    "    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/benchmark_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/benchmark_comparison.png\")\n",
    "\n",
    "plot_benchmark_comparison(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f1833",
   "metadata": {},
   "source": [
    "#### Visual Analysis of Benchmark Results\n",
    "\n",
    "The Figure presents a consolidated comparison of the four adversarial objectives (BCE, LSGAN, Hinge, and WGAN-GP) across fidelity metrics (FID, KID), computational cost, and training dynamics.\n",
    "\n",
    "##### FID Comparison (Top-Left)\n",
    "\n",
    "The FID bar chart confirms the quantitative ranking observed in the benchmark table:\n",
    "\n",
    "- LSGAN achieves the lowest FID (‚âà 6.3), indicating the best alignment between the generated and real MNIST distributions.\n",
    "\n",
    "- BCE follows closely (‚âà 7.7), remaining a strong baseline but slightly inferior to LSGAN.\n",
    "\n",
    "- Hinge loss shows a noticeable degradation (‚âà 10.4), suggesting that margin-based objectives are less well suited to this architecture and dataset scale.\n",
    "\n",
    "- WGAN-GP performs substantially worse (‚âà 68.0), reflecting poor sample fidelity under the current training configuration.\n",
    "\n",
    "The dashed horizontal line marking the best FID visually highlights the clear separation between LSGAN/BCE and the remaining objectives.\n",
    "\n",
    "##### KID Comparison with Variance (Top-Center)\n",
    "\n",
    "The KID plot (with standard deviation bars) mirrors the FID ranking:\n",
    "\n",
    "- LSGAN again yields the lowest KID (‚âà 0.0034 ¬± 0.0017), reinforcing its superior sample quality.\n",
    "\n",
    "- BCE remains competitive (‚âà 0.0048 ¬± 0.0020), with overlapping confidence intervals but consistently higher mean error than LSGAN.\n",
    "\n",
    "- Hinge exhibits higher divergence (‚âà 0.0070 ¬± 0.0022).\n",
    "\n",
    "- WGAN-GP shows both the highest mean KID (‚âà 0.069) and the largest variance, indicating unstable generation quality across subsets.\n",
    "\n",
    "The relatively small variance for BCE and LSGAN suggests stable convergence, while the larger spread for WGAN-GP is consistent with less stable training dynamics.\n",
    "\n",
    "##### Training Time (Top-Right)\n",
    "\n",
    "The training time plot highlights the computational cost of each objective:\n",
    "\n",
    "- BCE, LSGAN, and Hinge complete within a narrow range (‚âà 280‚Äì290 seconds).\n",
    "\n",
    "- WGAN-GP requires ‚âà 1,360 seconds, almost 5√ó longer than the other methods.\n",
    "\n",
    "This large overhead is explained by:\n",
    "\n",
    "- multiple critic updates per generator step (n_critic = 5),\n",
    "\n",
    "- gradient penalty computation at each discriminator update,\n",
    "\n",
    "- higher memory and compute cost per iteration.\n",
    "\n",
    "Crucially, this increased cost does not translate into better fidelity, making WGAN-GP dominated in this experimental setting.\n",
    "\n",
    "##### Generator Loss Curves (Bottom-Left)\n",
    "\n",
    "The generator loss trajectories reveal qualitative differences in optimization dynamics:\n",
    "\n",
    "- BCE and LSGAN exhibit smooth, gradually stabilizing loss curves, indicative of steady adversarial learning and convergence.\n",
    "\n",
    "- Hinge loss shows noisier behavior, with frequent oscillations around zero, reflecting the margin-based objective‚Äôs sensitivity to discriminator strength.\n",
    "\n",
    "- WGAN-GP displays highly volatile generator loss values, with large-magnitude oscillations over time. This behavior is consistent with the Wasserstein objective being sensitive to the balance between:\n",
    "\n",
    "1. critic capacity,\n",
    "\n",
    "2. gradient penalty strength,\n",
    "\n",
    "3. regularization mechanisms such as Spectral Normalization and Dropout.\n",
    "\n",
    "The instability observed here correlates with the degraded FID/KID results for WGAN-GP.\n",
    "\n",
    "##### Overall Comparison Radar Plot (Bottom-Right)\n",
    "\n",
    "The radar chart summarizes four dimensions: FID quality, KID quality, class-consistency, and speed.\n",
    "\n",
    "- LSGAN dominates across all axes, achieving strong fidelity, high class-consistency, and competitive speed.\n",
    "\n",
    "- BCE follows closely, slightly behind in fidelity but comparable in class-conditioning accuracy and computational efficiency.\n",
    "\n",
    "- Hinge shows weaker fidelity despite similar speed, indicating that its benefits do not materialize in this low-resolution conditional setting.\n",
    "\n",
    "- WGAN-GP is strongly penalized on both quality and speed, and also exhibits a marked drop in class-consistency, reflecting poorer alignment between generated digits and conditioned labels.\n",
    "\n",
    "##### Discussion and Implications\n",
    "\n",
    "Across all visual and quantitative indicators, LSGAN consistently provides the best trade-off between stability, fidelity, and computational efficiency. BCE remains a robust baseline, confirming that classical GAN objectives remain competitive when combined with architectural stabilization techniques (Spectral Normalization, TTUR, conditional inputs).\n",
    "\n",
    "The poor performance of WGAN-GP in this benchmark is particularly noteworthy. While theoretically appealing, WGAN-GP appears mismatched with the chosen regularization stack (Spectral Normalization + Dropout) and the low-resolution MNIST domain. The interaction between gradient penalties and spectral constraints likely over-regularizes the critic, weakening the learning signal provided to the generator. This highlights that WGAN-GP is not universally superior and requires careful adaptation of architecture and hyperparameters to outperform simpler objectives.\n",
    "\n",
    "These results justify the selection of LSGAN as the preferred adversarial objective for conditional MNIST generation in this project and underline the importance of empirical benchmarking when choosing GAN loss formulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abc91e",
   "metadata": {},
   "source": [
    "### 8.  Per Class Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class quality analysis\n",
    "\n",
    "def analyze_per_class_quality(results, n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate samples for each class and compute per-class statistics.\n",
    "    Helps identify if certain digits are harder to generate.\n",
    "    \"\"\"\n",
    "    print(\"\\nPer-Class Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    class_stats = {s: {} for s in results.keys()}\n",
    "    \n",
    "    for strategy in results.keys():\n",
    "        # Load the trained generator\n",
    "        g_model = Generator().to(device)\n",
    "        g_model.load_state_dict(results[strategy]['g_model_state'])\n",
    "        g_model.eval()\n",
    "        \n",
    "        for digit in range(10):\n",
    "            noise = torch.randn(n_samples, LATENT_DIM, device=device)\n",
    "            labels = torch.full((n_samples, 1), digit, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                samples = g_model(noise, labels)\n",
    "            \n",
    "            # Compute statistics\n",
    "            mean_intensity = samples.mean().item()\n",
    "            std_intensity = samples.std().item()\n",
    "            \n",
    "            class_stats[strategy][digit] = {\n",
    "                'mean': mean_intensity,\n",
    "                'std': std_intensity,\n",
    "            }\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(f\"\\n{'Digit':<8}\", end=\"\")\n",
    "    for s in results.keys():\n",
    "        print(f\"{s.upper():>12}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * (8 + 12 * len(results)))\n",
    "    \n",
    "    for digit in range(10):\n",
    "        print(f\"{digit:<8}\", end=\"\")\n",
    "        for s in results.keys():\n",
    "            std = class_stats[s][digit]['std']\n",
    "            print(f\"{std:>12.3f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n(Values show standard deviation - higher = more variety)\")\n",
    "\n",
    "analyze_per_class_quality(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0205c2",
   "metadata": {},
   "source": [
    "Per-Class Diversity Analysis\n",
    "\n",
    "Table X reports the per-class standard deviation of generated samples for each digit (0‚Äì9) under the four adversarial objectives. Higher values indicate greater intra-class diversity, i.e., a wider variety of writing styles, stroke thicknesses, and shapes within the same digit class.\n",
    "\n",
    "##### Comparison Across Loss Functions\n",
    "\n",
    "Across nearly all digits, BCE and LSGAN consistently achieve the highest intra-class diversity, with very similar magnitudes. For instance:\n",
    "\n",
    "- Digit 0: BCE ‚âà 0.673 vs. LSGAN ‚âà 0.674\n",
    "\n",
    "- Digit 3: BCE ‚âà 0.626 vs. LSGAN ‚âà 0.602\n",
    "\n",
    "- Digit 8: BCE ‚âà 0.629 vs. LSGAN ‚âà 0.608\n",
    "\n",
    "This indicates that both objectives are capable of producing stylistically diverse samples within each class, rather than collapsing to a narrow prototype. The marginal advantage of BCE on some digits (e.g., 2, 3, 6, 8) is small and does not change the overall ranking.\n",
    "\n",
    "Hinge loss exhibits systematically lower diversity across most classes. While the drop is moderate, it is consistent (e.g., digit 4: 0.548 vs. BCE 0.568; digit 7: 0.550 vs. BCE 0.568), suggesting a mild tendency toward more concentrated modes. This aligns with earlier observations that hinge objectives can favor sharper but less diverse outputs when model capacity or regularization is limited.\n",
    "\n",
    "WGAN-GP displays the lowest diversity for every digit, with a substantial margin. For example:\n",
    "\n",
    "- Digit 1: WGAN-GP ‚âà 0.362 vs. BCE/LSGAN ‚âà 0.473\n",
    "\n",
    "- Digit 4: WGAN-GP ‚âà 0.466 vs. BCE/LSGAN ‚âà 0.57\n",
    "\n",
    "- Digit 9: WGAN-GP ‚âà 0.455 vs. BCE ‚âà 0.577\n",
    "\n",
    "This pattern indicates a pronounced mode contraction under WGAN-GP in this setup, where generated samples for each class cluster around a limited set of visual patterns. This is consistent with the previously observed degradation in FID/KID and the unstable generator dynamics for WGAN-GP.\n",
    "\n",
    "##### Class-Specific Observations\n",
    "\n",
    "Some digits appear intrinsically easier to diversify across all objectives:\n",
    "\n",
    "- Digits 0, 3, 6, and 8 consistently show higher diversity across BCE, LSGAN, and Hinge. These digits naturally admit multiple writing styles (loops, stroke openings, curvature), which the models successfully capture.\n",
    "\n",
    "- Digit 1 exhibits the lowest diversity across all methods, reflecting the limited stylistic variability of the digit ‚Äú1‚Äù in MNIST (mostly vertical strokes with minor variations). The gap between WGAN-GP and the other methods is particularly pronounced here, indicating near-prototype collapse.\n",
    "\n",
    "##### Interpretation and Link to Global Metrics\n",
    "\n",
    "The per-class diversity trends reinforce the conclusions drawn from FID/KID and class-consistency:\n",
    "\n",
    "- LSGAN and BCE achieve a favorable balance between fidelity and diversity, producing not only realistic digits but also a wide range of styles within each class.\n",
    "\n",
    "- Hinge loss trades off some diversity for sharper but more constrained generation, which may explain its weaker global FID/KID.\n",
    "\n",
    "- WGAN-GP suffers from both low fidelity and low diversity, suggesting that, under the current architecture and regularization regime (Spectral Normalization + Dropout + Gradient Penalty), the critic becomes overly constrained, leading to a weak learning signal and partial mode collapse.\n",
    "\n",
    "This per-class analysis provides important complementary evidence that the superiority of LSGAN (and the competitiveness of BCE) is not limited to global distributional metrics but extends to intra-class variability, which is crucial for conditional generation tasks. Conversely, the consistently low per-class diversity of WGAN-GP explains, at a qualitative level, its poor FID/KID scores and supports the conclusion that WGAN-GP is ill-suited to the present experimental configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea1767",
   "metadata": {},
   "source": [
    "### 8.1 Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated Samples Comparison Grid\n",
    "\n",
    "def plot_samples_comparison(results):\n",
    "    \"\"\"Show final generated samples from each strategy side by side.\"\"\"\n",
    "    strategies = list(results.keys())\n",
    "    n_strategies = len(strategies)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, n_strategies * 4, figsize=(n_strategies * 5, 5))\n",
    "    \n",
    "    for col, strategy in enumerate(strategies):\n",
    "        samples = results[strategy]['final_samples']\n",
    "        labels = results[strategy]['labels_test']\n",
    "        \n",
    "        for i in range(16):\n",
    "            row = i // 4\n",
    "            sub_col = i % 4\n",
    "            ax = axes[row, col * 4 + sub_col]\n",
    "            \n",
    "            img = samples[i].numpy().reshape(28, 28)\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Add strategy name as title for first row\n",
    "            if i == 0:\n",
    "                ax.set_title(f'{strategy.upper()}\\n{labels[i].item()}', fontsize=10)\n",
    "            elif row == 0:\n",
    "                ax.set_title(str(labels[i].item()), fontsize=9)\n",
    "    \n",
    "    fig.suptitle('Generated Samples Comparison', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/samples_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/samples_comparison.png\")\n",
    "\n",
    "plot_samples_comparison(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a32950",
   "metadata": {},
   "source": [
    "Figure X ‚Äî Qualitative comparison of generated MNIST samples across loss strategies.\n",
    "\n",
    "The grid illustrates representative conditional samples produced by each model after training. BCE and LSGAN generate clear, well-formed digits with consistent class alignment and visible stylistic variation. Hinge loss produces recognizable digits but with slightly reduced smoothness and higher intra-class variability in stroke continuity. In contrast, WGAN-GP exhibits visibly degraded samples, with less stable digit structure and occasional artifacts, reflecting the weaker quantitative performance observed under this training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf387195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export benchmark results to CSV\n",
    "\n",
    "def export_results_csv(results, filename='benchmark_results.csv'):\n",
    "    \"\"\"Export benchmark results to CSV for further analysis.\"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Strategy', 'FID', 'KID_Mean', 'KID_Std', 'Training_Time_Seconds'])\n",
    "        \n",
    "        for name, data in results.items():\n",
    "            writer.writerow([\n",
    "                name,\n",
    "                f\"{data['fid']:.4f}\",\n",
    "                f\"{data['kid_mean']:.6f}\",\n",
    "                f\"{data['kid_std']:.6f}\",\n",
    "                f\"{data['training_time']:.2f}\"\n",
    "            ])\n",
    "    \n",
    "    print(f\"Results exported to {filename}\")\n",
    "\n",
    "# export_results_csv(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22825292",
   "metadata": {},
   "source": [
    "# 8. Per-class Grid\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdd148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class(g_model, rows_per_class=5, title=\"Generated Samples Per Class\"):\n",
    "    \"\"\"Generates a grid with one column per digit class (0-9).\"\"\"\n",
    "    fig, axes = plt.subplots(rows_per_class, 10, figsize=(15, 8))\n",
    "\n",
    "    for digit in range(10):\n",
    "        noise = torch.randn(rows_per_class, LATENT_DIM, device=device)\n",
    "        labels = torch.full((rows_per_class,), digit, device=device).long()  # shape: [B]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = g_model(noise, labels)\n",
    "\n",
    "        for row in range(rows_per_class):\n",
    "            img = images[row].cpu().numpy().reshape(28, 28)\n",
    "            axes[row, digit].imshow(img, cmap='gray')\n",
    "            axes[row, digit].axis('off')\n",
    "\n",
    "            if row == 0:\n",
    "                axes[row, digit].set_title(str(digit))\n",
    "\n",
    "    fig.suptitle(title, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot for all strategies\n",
    "for strategy_name in benchmark_results:\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(benchmark_results[strategy_name]['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    plot_per_class(g_model, title=f\"Generated Samples Per Class ‚Äî {strategy_name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Fixed Z, Varying Label Visualization\n",
    "# =============================================================================\n",
    "# Demonstrates controllability: same noise vector produces different digits\n",
    "# when conditioned on different labels.\n",
    "\n",
    "def plot_fixed_z_varying_label(g_model, n_rows=4, title=\"Fixed Z, Varying Label (0-9)\"):\n",
    "    \"\"\"\n",
    "    Generate images using fixed noise vectors but varying labels.\n",
    "    Each row uses the same z, columns show digits 0-9.\n",
    "    \n",
    "    This visually demonstrates that the cGAN has learned to use the label\n",
    "    to control which digit is generated, independent of the noise vector.\n",
    "    \"\"\"\n",
    "    g_model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 10, figsize=(15, n_rows * 1.5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row in range(n_rows):\n",
    "            # Fix one noise vector for this row\n",
    "            fixed_z = torch.randn(1, LATENT_DIM, device=device)\n",
    "            \n",
    "            for digit in range(10):\n",
    "                label = torch.tensor([digit], device=device).long()\n",
    "                \n",
    "                # Generate with fixed z, varying label\n",
    "                sample = g_model(fixed_z, label)\n",
    "                \n",
    "                img = sample[0].cpu().numpy().reshape(28, 28)\n",
    "                axes[row, digit].imshow(img, cmap='gray')\n",
    "                axes[row, digit].axis('off')\n",
    "                \n",
    "                # Column headers (digit labels)\n",
    "                if row == 0:\n",
    "                    axes[row, digit].set_title(str(digit), fontsize=12)\n",
    "            \n",
    "            # Row label\n",
    "            axes[row, 0].set_ylabel(f'z_{row+1}', fontsize=10, rotation=0, labelpad=20)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('images', exist_ok=True)\n",
    "    plt.savefig('images/fixed_z_varying_label.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: images/fixed_z_varying_label.png\")\n",
    "\n",
    "\n",
    "# Plot for all strategies\n",
    "for strategy_name in benchmark_results:\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(benchmark_results[strategy_name]['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    plot_fixed_z_varying_label(g_model, n_rows=4, title=f\"Fixed Z, Varying Label ‚Äî {strategy_name.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8817863c",
   "metadata": {},
   "source": [
    "# 10. Model Saving\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8507f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(results):\n",
    "    \"\"\"Saves generator and discriminator state dicts for all strategies.\"\"\"\n",
    "    os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    for strategy_name, data in results.items():\n",
    "        g_path = os.path.join(MODEL_OUTPUT_PATH, f'G_{strategy_name}.pt')\n",
    "        d_path = os.path.join(MODEL_OUTPUT_PATH, f'D_{strategy_name}.pt')\n",
    "        \n",
    "        torch.save(data['g_model_state'], g_path)\n",
    "        torch.save(data['d_model_state'], d_path)\n",
    "        \n",
    "        print(f\"Saved: {g_path}, {d_path}\")\n",
    "\n",
    "save_models(benchmark_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63e5cf",
   "metadata": {},
   "source": [
    "# 11. Single-Image Inference\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a digit from 0-9\n",
    "digit = 8\n",
    "\n",
    "# Use same noise for fair comparison\n",
    "z = torch.randn(1, LATENT_DIM, device=device)\n",
    "label = torch.tensor([[digit]], device=device)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(benchmark_results), figsize=(3 * len(benchmark_results), 3))\n",
    "\n",
    "for idx, (strategy_name, data) in enumerate(benchmark_results.items()):\n",
    "    g_model = Generator().to(device)\n",
    "    g_model.load_state_dict(data['g_model_state'])\n",
    "    g_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = g_model(z, label)\n",
    "    \n",
    "    img = generated[0].cpu().numpy().reshape(28, 28)\n",
    "    axes[idx].imshow(img, cmap='gray')\n",
    "    axes[idx].set_title(f\"{strategy_name.upper()}\\nFID: {data['fid']:.1f}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.suptitle(f\"Generated Digit: {digit}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
